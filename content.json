{"pages":[],"posts":[{"title":"两数之和","text":"问题描述12345678910111213141516171819202122给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。 示例 1：输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。示例 2：输入：nums = [3,2,4], target = 6输出：[1,2]示例 3：输入：nums = [3,3], target = 6输出：[0,1] 解答12345678910111213public int[] twoSum(int[] nums, int target) { int[] res = new int[2]; for (int i = 0; i &lt; nums.length; i++) { for (int j = (i + 1); j &lt; nums.length; j++) { if (nums[j] + nums[i] == target) { res[0] = i; res[1] = j; } } } return res;}","link":"/2022/05/29/LeetCode/1.%20%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"title":"微服务介绍","text":"微服务介绍 一、为什么要用微服务？ 互联网应用的特点： 需求变化快 用户群体大二、 什么是微服务？ 简单来说，微服务就是将一种单一应用程序拆分为一组小型服务的方法，拆分完成之后，每一个服务都运行在独立的进程中，服务之间采用轻量级的通信机制来沟通。SpringCloud 中采用基于 HTTP 的 RESTful API 三、微服务的优势 复杂度可控 独立部署 技术选型灵活 较好的容错性 较强的可扩展性 四、 Spring Cloud 核心特性 服务注册与发现 负载均衡 服务之间调用 容错、服务降级、断路器 消息总线 分布式配置中心 链路器","link":"/2022/04/01/springCloud/1.%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BB%8B%E7%BB%8D/"},{"title":"两数相加","text":"问题 123456789101112131415161718192021222324252627282930313233给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。请你将两个数相加，并以相同形式返回一个表示和的链表。你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例 1：输入：l1 = [2,4,3], l2 = [5,6,4]输出：[7,0,8]解释：342 + 465 = 807.示例 2：输入：l1 = [0], l2 = [0]输出：[0]示例 3：输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9]输出：[8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 [1, 100] 内 0 &lt;= Node.val &lt;= 9 题目数据保证列表表示的数字不含前导零 解法1 失败1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { Long n1 = nodeNum(reserver(l1)); Long n2 = nodeNum(reserver(l2)); Long n3 = n1 + n2; int n4 = pan(n3); return get(n3,n4); } ListNode get(Long a,int count){ ListNode listNode = new ListNode(-1); int flag = count; for (int i = 0; i &lt; count; i++) { flag -- ; if (flag &lt; 0){ return listNode.next; } Double b = Math.pow(10,flag); Long c = a/(b.longValue()); ListNode temp = listNode.next; listNode.next = new ListNode(c.intValue()); listNode.next.next = temp; a = a - b.longValue() * c; } return listNode.next; } ListNode reserver(ListNode old){ ListNode newList = new ListNode(-1); while (old != null){ ListNode temp = old.next; old.next = newList.next; newList.next = old; old = temp; } return newList.next; } int pan(Long num){ int count = 0; while (num &gt;=10){ num /= 10; count ++; } return count +1 ; } Long nodeNum(ListNode l1){ StringBuilder sb = new StringBuilder(); while (l1 != null){ sb.append(l1.val); l1 = l1.next; } return Long.valueOf(sb.toString()); }} 本来想着通过数字计算方式的。真恶心 这个数字 解法212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.math.BigInteger;class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { BigInteger n1 = nodeTONum(reserver(l1)); BigInteger n2 = nodeTONum(reserver(l2)); BigInteger n3 = n1.add(n2); int n4 = pan(n3); return getNode(n3,n4); } // 链表转数字 BigInteger nodeTONum(ListNode l1){ StringBuilder sb = new StringBuilder(); while (l1 != null){ sb.append(l1.val); l1 = l1.next; } return new BigInteger(sb.toString()); } // 获取位数 int pan(BigInteger num){ int count = 0; BigInteger flag = new BigInteger(&quot;10&quot;); while (num.compareTo(flag)!=-1){ num = num.divide(flag); count ++; } return count +1 ; } ListNode getNode(BigInteger a,int count){ ListNode listNode = new ListNode(-1); int flag = count; BigInteger flag1= new BigInteger(&quot;10&quot;); for (int i = 0; i &lt; count; i++) { flag -- ; if (flag &lt; 0){ return listNode.next; } BigInteger b = flag1.pow(flag); BigInteger c = a.divide(b); ListNode temp = listNode.next; listNode.next = new ListNode(c.intValue()); listNode.next.next = temp; a = a.subtract((b.multiply(c))); } return listNode.next; } ListNode reserver(ListNode old){ ListNode newList = new ListNode(-1); while (old != null){ ListNode temp = old.next; old.next = newList.next; newList.next = old; old = temp; } return newList.next; }}","link":"/2022/05/29/LeetCode/2.%20%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"},{"title":"服务注册中心","text":"服务注册中心EurekaEureka 是 Spring Cloud 的注册中心，类似 Dubbo 中的 Zookeeper。 到底什么是注册中心？我们为什么要注册中心 在单体应用中, 所有的业务集中在一个项目中，调用过程是一条直线，不需要服务之间的中转，没有必要引入注册中心 项目越来越大，我们会将系统进行拆分。各个模块之间进行协作，模块之间的调用 会变得越来越复杂，模块之间存在强耦合。 为了解决服务之间的耦合，注册中心方案自然就出现了。 基于 REST 来实现服务的注册与发现。 Eureka 是 Spring Cloud最重要的核心组件。 Eureka 提供可视化的页面，可以方便的查看服务注册情况医技服务注册中心集群的运行情况。 从图中可以看出,存在3个角色 Eureka Server 注册中心 Eureka Provider 服务提供者 Eureka Consumer 服务消费者 Eureka 环境搭建 创建一个普通的 spring boot项目, 添加 Eureka 依赖 项目创建成功之后，在项目启动类上添加注解，表示项目是一个 Eureka Server 12345678910@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication { public static void main(String[] args) { SpringApplication.run(EurekaApplication.class, args); }} 其中 @EnableEurekaServer 注解表示开启 Eureka 功能 123456789# 给单前服务取名spring.application.name=eureka# 端口号server.port=1111# 表示当前项目不要注册到中心上eureka.client.register-with-eureka=false# 表示是否从 Eureka Server 获取注册信息eureka.client.fetch-registry=false 配置完成后就可以启动了 Eureka 集群搭建使用了注册中心之后，所有的服务都需要通过服务注册中心来进行交互。服务的注册中心的稳定性就非常重要了。所以注册中心一般都是集群的形式。 Eureka 集群就是多个 实例，相互注册，互相同步数据，组成集群实例A实例B配置完成后 打包启动A 1java -jar .\\eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=a 启动B 1java -jar .\\eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=b 互相注册成功 Eureka 工作细节本身可以分为2大部分：Eureka Server 和 Eureka Client Eureka Server 服务注册，所有服务都注册到 Eureka Server 上来 提供注册表，注册表是所有注册上来服务的列表，Eureka Client调用服务时,需要获取注册表，一般来说，注册表会缓存下来。 无状态，Eureka Server 通过注册心跳机制，和 Eureka Client同步 客户端状态。 Eureka ClientEureka Client 用来简化 服务和 Eureka Server 之间的交互,自动拉取，更新以及缓存 Eureka Server 的信息。即使 Eureka Server 宕机了，依然能够获取想要调用服务的地址（地址可能不准确） 服务注册所谓 Eureka Server 只是一个业务上划分，其实本身也是一个 Eureka Client 。需要把自身的元数据暴露出去,比如 ip,端口 运行状态等 服务续约默认情况下,服务注册之后，Eureka Client 每隔 30s 就要向 Eureka Server 发送一条心跳消息。Eureka Server 90s 内 没有收到 Eureka Client的消息，就会认为 Eureka Client 下线，会从服务列表中移除。一般不建议修改。 12eureka.instance.lease-renewal-interval-in-seconds=30eureka.instance.lease-expiration-duration-in-seconds=90 服务下线Eureka Client 下线周，会主动发送消息 告诉 Eureka Server ,我要下线了 获取注册表信息Eureka Client 从 Eureka Server 上获取服务注册信息，缓存到本地。数据会定期更新。2个属性 1234# 是否准许获取注册表信息eureka.client.fetch-registry=true# 更新时间间隔eureka.client.registry-fetch-interval-seconds=30 Eureka 集群原理官方架构图 Eureka Server 之间通过 Replicte 同步数据，不同的Eureka Server 不区分主从节点，所有节点平等的。节点之间，通过 servceUrl 互相注册，形成集群 A –&gt; B –&gt; CA 也是 能够和 C同步，不建议","link":"/2022/04/01/springCloud/2.%20%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/"},{"title":"服务注册与消费","text":"服务注册与消费一、服务注册微服务注册到 Eureka Server 上,这样其他服务需要调用该服务时,只需要从 Eureka Server 上查询服务的信息即可 创建一个 provider 作为服务提供者,创建项目时，选择 Eureka Client 依赖和 WEB依赖。这样服务创建成功后，简单配置一下 就可以使用了 在 propertires 中 配置项目注册地址 123spring.application.name=providerserver.port=1114eureka.client.service-url.defaultZone = http://localhost:1111/eureka 启动 Erueka Server 再启动 provider：注册成功！ 二、服务消费2.1 基本用法在provider 中提供一个 hello 接口 1234567@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String hello(){ return &quot;hello provider&quot;; }} 再创建一个 consumer项目，去消费 provider 提供的接口。consumer 只要能够获取到 provider 接口的地址，就需要 再 consumer 中写死地址，意味着服务之间的耦合度太高了 创建 consumer 项目，添加 web 和 eureka client 依赖 项目创建后,添加配置信息 123spring.application.name=consumerserver.port=1115eureka.client.service-url.defaultZone = http://localhost:1111/eureka 实际调用写死路径 12345678910111213141516171819202122232425@RestControllerpublic class UseHelloController { @GetMapping(&quot;/hello1&quot;) public String hello1(){ HttpURLConnection con = null; URL url = null; try { url = new URL(&quot;http://localhost:1114/hello&quot;); con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String s = br.readLine(); br.close(); return s; } } catch (MalformedURLException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return &quot;xxxxxx&quot;; }} 从 Eureka 服务发现中 请求接口 12345678910111213141516171819202122232425262728293031@RestControllerpublic class UseHelloController { @Autowired DiscoveryClient discoveryClient; @GetMapping(&quot;/hello2&quot;) public String hello2(){ List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;provider&quot;); ServiceInstance instance = list.get(0); String ip = instance.getHost();//ip int port = instance.getPort();//端口 String Url = &quot;http://&quot; + ip + &quot;:&quot; +port + &quot;/hello&quot;; //从服务发现实例中获取地址 HttpURLConnection con = null; URL url = null; try { url = new URL(Url); con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String s = br.readLine(); br.close(); return s; } } catch (MalformedURLException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return &quot;xxxxxx&quot;; }} 注意 DiscoveryClient 查询的服务列表是一个集合，其中每一个项就是一个服务实例 2.2 升级改造Http 调用, 从 RestTemplate 实现 12345678910111213141516171819@BeanRestTemplate restTemplateOne(){ return new RestTemplate();}@AutowiredRestTemplate restTemplate;@GetMapping(&quot;/hello3&quot;)public String hello3(){ List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;provider&quot;); ServiceInstance instance = list.get(0); String ip = instance.getHost();//ip int port = instance.getPort();//端口 String Url = &quot;http://&quot; + ip + &quot;:&quot; +port + &quot;/hello&quot;; //从服务发现实例中获取地址 String s = restTemplate.getForObject(Url,String.class); return s;} 接下来 使用 Ribbon 实现负载均衡首先，我们需要给 RestTemplate 添加注解 12345678910111213141516@Bean@LoadBalancedRestTemplate restTemplateTwo(){ return new RestTemplate();}@Autowired@Qualifier(&quot;restTemplateTwo&quot;)RestTemplate restTemplateTwo;@GetMapping(&quot;/hello4&quot;)public String hello4(){ String s = restTemplateTwo.getForObject(&quot;http://provider/hello&quot;,String.class); return s;} 负载均衡时，提供的地址是模糊的地址。不能给具体的地址 会报错。模糊地址会自动被拦截，自动实现负载均衡 三、RestTmplateRestTmplate 是从 sprng 3.0 开始 支持的 http 请求工具。提供常见 REST 请求方法模板：GET POST PUT DELETERestTmplate 实现了 RestOperation 接口，RestOperation定义了 RESTFUL 操作 3.1 GETgetForObject 返回的是一个对象，对象是返回的具体值。3种请求方式 12345678910//1String s = restTemplateOne.getForObject(&quot;http://provider/hello?name={1}&quot;,String.class,&quot;test&quot;);//2Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();map.put(&quot;name&quot;,&quot;test&quot;);String s1 = restTemplateOne.getForObject(&quot;http://provider/hello?name={name}&quot;,String.class,map);//3String url = &quot;http://provider/hello?name=&quot; + &quot;test&quot;;URI url1 = URI.create(url);String s2 = restTemplateOne.getForObject(url1,String.class); getForEntity 返回是一个 ResponseEntity 3.2 POSTprovider 中提供2个POST接口，同时 POST 请求需要传递 json。需要创建 maven 项目 作为 commons 模块。模块被 provider 和 consummer 共同引用。这样就可以方便传递 JSON了 provider 提供2个 post 接口 12345678910@PostMapping(&quot;/user1&quot;)public User addUser1(User user){//传参是以 key/value return user;}@PostMapping(&quot;/user2&quot;)public User addUser2(@RequestBody User user){//传参是以 json return user;} consumer 调用接口 1234567891011121314151617@GetMapping(&quot;/hello6&quot;) public String hello6(){ //1 key vaues 传递参数 MultiValueMap&lt;String,Object&gt; map = new LinkedMultiValueMap&lt;String,Object&gt;(); map.add(&quot;id&quot;,123); map.add(&quot;username&quot;,&quot;java&quot;); map.add(&quot;password&quot;,&quot;123&quot;); String url = &quot;http://provider/user1&quot;; User user = restTemplateTwo.postForObject(url,map,User.class); System.out.println(user); user.setId(456); String url2 = &quot;http://provider/user2&quot;; //2 json 传递参数 User user1 = restTemplateTwo.postForObject(url2,user,User.class); System.out.println(user1); return &quot;ok!!!&quot;; } 接口调用成功 postForLocation：场景，执行一个 post 请求之后, 马上进行重定向，比如注册完成之后，马上重定向到登录页面登录。http请求状态码 302 provider 提供注册接口,注意这里的post接口 响应码一定是302 不然无效 注意：重定向的地址一定要写成绝对路径，不要写成相对路径，否则调用会出问题，找不到服务实例 123456789101112@Controllerpublic class RegisterController { @PostMapping(&quot;/register&quot;) public String register(User user){ return &quot;redirect:http://provider/loginPage?name=&quot; + user.getUsername(); } @GetMapping(&quot;/loginPage&quot;) @ResponseBody public String loginPage(String name){ return &quot;loginPage:&quot; + name; }} consummer 调用 123456789101112131415@Autowired@Qualifier(&quot;restTemplateTwo&quot;)RestTemplate restTemplateTwo;@GetMapping(&quot;/hello7&quot;)public String hello7(){ String url = &quot;http://provider/register&quot;; MultiValueMap&lt;String,Object&gt; map = new LinkedMultiValueMap&lt;String,Object&gt;(); map.add(&quot;id&quot;,123); map.add(&quot;username&quot;,&quot;java&quot;); map.add(&quot;password&quot;,&quot;123&quot;); URI uri = restTemplateTwo.postForLocation(url,map); String s = restTemplateTwo.getForObject(uri,String.class); return s;} 负载均衡原理 从 Eureka Cilent 本地缓存的服务注册信息中选择一个可以调用的服务 根据选中的服务，重构请求的 URL 地址 最终将功能嵌入到 RestTemplate 中","link":"/2022/04/02/springCloud/3.%20%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E6%B6%88%E8%B4%B9/"},{"title":"Consul注册中心","text":"Consul注册中心备选方案。Consul主要提供了 服务发现、服务隔离、服务配置等功能相比于 Eureka 和 zookeeper，Consul 配置更加一站式,因为内置了很多微服务的常见的需求：服务发现与注册,分布式一致性协议实现、健康检查、键值对存储、多数据中心等、不在需要第三方组件来实现这些功能。 一、安装Consul使用go 语言开发,需要安装下载 1weget https://releases.hashicorp.com/consul/1.11.4/consul_1.11.4_linux_amd64.zip 安装 直接解压即可 1unzip consul_1.11.4_linux_amd64.zip 启动(开发模式启动,地址是本地虚拟机地址) 1./consul agent -dev -ui -node=consul-dev -client=192.168.10.128 启动成功之后，在物理机可以访问（需要注意端口 8500开放） 二、Consul 使用创建 spring boot 项目添加依赖 开启服务发现功能 123456789@SpringBootApplication@EnableDiscoveryClientpublic class ConsulProviderApplication { public static void main(String[] args) { SpringApplication.run(ConsulProviderApplication.class, args); }} 添加相关consul配置 123456spring.application.name=consul-providerserver.port=2000#consul 相关配置spring.cloud.consul.host=192.168.10.128spring.cloud.consul.port=8500spring.cloud.consul.discovery.service-name=consul-provider 添加测试接口 1234567@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String hello(){ return &quot;hello&quot;; }} 项目启动,注册成功 三、Consul 集群使用为了区别增加端口打印 123456789@RestControllerpublic class HelloController { @Value(&quot;${server.port}&quot;) Integer port; @GetMapping(&quot;/hello&quot;) public String hello(){ return &quot;hello :&quot; + port; }} 对项目进行打包,在命令行执行命令,启动多个实例 123java -jar .\\consul-provider-0.0.1-SNAPSHOT.jar --server.port=2001java -jar .\\consul-provider-0.0.1-SNAPSHOT.jar --server.port=2000 查看 consul 管理界面，集群注册成功 四、consul 消费搭建环境 consul-consumer 创建成功后，添加一下配置 123456spring.application.name=consul-consumerserver.port=2005#consul 相关配置spring.cloud.consul.host=192.168.10.128spring.cloud.consul.port=8500spring.cloud.consul.discovery.service-name=consul-consumer 开启服务发现 1@EnableDiscoveryClient 提供一个服务调用方法 12345678910111213141516171819@RestControllerpublic class HelloController { @Autowired LoadBalancerClient loadBalancerClient; @Autowired RestTemplate restTemplate; @Bean RestTemplate getRstTemplate(){ return new RestTemplate(); } @GetMapping(&quot;/hello&quot;) public void hello(){ ServiceInstance choose = loadBalancerClient.choose(&quot;consul-provider&quot;); System.out.println(&quot;服务地址&quot; + choose.getUri()); System.out.println(&quot;服务名称&quot; + choose.getServiceId()); String forObject = restTemplate.getForObject(choose.getUri() + &quot;/hello&quot;, String.class); System.out.println(forObject); }}","link":"/2022/04/03/springCloud/4.%20consul/"},{"title":"Hystrix","text":"Hystrix1. 基本介绍断路器。微服务系统中，系统出错的概率非常高，在微服务系统中，涉及的模块很多,每个模块出错，都会导致整个系统不可用。希望在整个系统中，某个模块无法正常工作时，能够通过提前配置一些东西，使得整个系统正常运行。 2. 简单使用\\容错\\服务降级创建module体检依赖 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;2.2.10.RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加配置 123456# 给服务取名spring.application.name=hystrix# 端口server.port=3000# 注册服务eureka.client.service-url.defaultZone = http://localhost:1111/eureka 开启 hystrix 1@EnableHystrix 服务降级服务容错 12345678910111213141516171819@Servicepublic class HelloService { @Autowired RestTemplate restTemplate; @HystrixCommand(fallbackMethod = &quot;error&quot;) public String hello(){ return restTemplate.getForObject(&quot;http://provider/hello&quot;,String.class); } /** * 服务降级方法 注意要和 fallbackMethod 配置的 vlues 要一致 返回类型也要一致 * @return */ public String error(){ return &quot;error&quot;; }} 实际请求 12345678910@RestControllerpublic class HelloController { @Autowired HelloService helloService; @GetMapping(&quot;/hello&quot;) public String hello(){ return helloService.hello(); }} 启动 Euraka 注册中心 启动 provider 服务 访问 hello 接口 正常访问 异常访问 服务降级 3. 请求命令4. 异常处理发起服务调用时，如果不是 provider 的原因导致服务异常。而是 consumer 本身问题 导致的异常，consumer 5. 请求缓存6. 请求合并","link":"/2022/04/04/springCloud/5.%20hystrix/"},{"title":"properties","text":"1. properties 对于 properties 格式的配置文件，可以随意自定义配置文件，例如在 resources 目录下新建一个 data.properties 的配置文件来配置数据库信息，像这样 123db.username=rootdb.password=123db.url=jdbc:mysql://xxxxx 然后在 java 代码中, 利用 @PropertySource 和 @ConfigurationProperties 两个注解来加载配置文件： 12345678@PropertySource(&quot;classpath:data.properties&quot;)@ConfigurationProperties(prefix = &quot;db&quot;)public class DbProperties { private String username; private String password; private String url; //省略 getter/setter} @PropertySource 就是用来加载一个自定义的 properties 配置文件 2. Yaml 对于 YAML 格式的配置而言，它其实是没有一个类似于 @PropertySource 注解的东西的，也就是说 YAML 配置，你只能写在 application.yaml 中，不能写在其他地方。要是写在其他地方，就没法加载了，其实也不是没法加载，可以自己调用 snakeyaml 包中的类去加载，但是那样太费事了，所以我们期望能够有一个简便的办法来做这件事。 怎么做呢？ 我们可以利用 Spring Boot 中对于 profile 多环境的处理方式来加载自定义的 YAML 配置。 大家知道，Spring Boot 中对于多环境的处理方式非常方便，以 yaml 配置为例，我们只需要定义一个名为 application-{profile}.yaml 的配置文件，然后在 application.yaml 中就可以指定这个环境了，利用这个漏洞，就可以非常方便的加载自定义的 yaml 配置了，不过这种方式对自定义的 yaml 配置文件名格式有要求，就是必须是 application-{profile}.yaml 格式的。 我举一个简单例子吧，例如对于数据库的配置，我想单独搞一个配置文件，那么我们可以新建一个 application-data.yaml 的配置文件，内容如下： 12345spring: datasource: password: 123 username: root url: jdbc:mysql:///xxxx 然后在 application.yaml 中就像设置环境一样引入这个配置文件，如下： 123spring: profiles: active: data 这样就实现了对自定义 yaml 配置文件的加载。 如果有多个自定义的 yaml 配置，那么也可以一起加载，多个环境之间用 , 隔开即可。","link":"/2022/04/22/springBoot/peoperties%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"title":"springBean 生命周期","text":"一、bean的生命周期1.1 普通 java 环境 java 源码编译为 class 文件 等到类被需要初始化(new,反射) class 文件被虚拟机通过类加载器到 JVM 初始化对象给调用者使用 简单来说是用 Class 对象作为模板创建的对象实例 1.2 spring 管理BeanDefinition 实例描述对象信息 spring 启动时 扫描 XML\\注解\\JavaConifg 中需要被 Spring 管理的 Bean 信息 描述信息封装成 BeanDefinition ，最终放到一个 BeanDefinitionMap 中。key 就是 beanName,value 是BeanDefinition 执行 BeanFactoryPostProcessor 前置处理器 对象实例化 创建对象。此时对象的属性是没有注入的 实例化 –》 依赖注入 ApplicationContextAware接⼝ 增强类 获取 取ApplicationContext对象 执行 BeanPostProcessor 后置处理器 [AOP 关键] before after 解决循环依赖：AB对象。A实例化 发现 依赖B对象。B对象此时还没创建出来，所以转头去实例化B对象。","link":"/2022/05/04/springBoot/spring%20bean%20%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/"},{"title":"全局统一返回数据","text":"全局统一结果返回降低开发人员沟通成本，返回结果统一约定统一格式。 1. 定义状态码12345678910111213141516171819202122public enum ResultCode { RS200(200,&quot;操作成功&quot;), RS500(500,&quot;操作失败&quot;); // 自定义状态码 private final int code; // 自定义描述 private final String msg; ResultCode(int code, String msg){ this.code = code; this.msg = msg; } public int getCode() { return code; } public String getMsg() { return msg; }} 2. 定义返回对象123456789101112131415161718192021222324252627282930313233import lombok.Data;@Datapublic class BaseResult&lt;T&gt; { int code; long time; String msg; T body; public BaseResult(){ this.time = System.currentTimeMillis(); } public static &lt;T&gt; BaseResult&lt;T&gt; success(T body){ BaseResult result = new BaseResult(); result.setCode(ResultCode.RS200.getCode()); result.setMsg(ResultCode.RS200.getMsg()); result.setBody(body); return result; } public static &lt;T&gt; BaseResult&lt;T&gt; fail(){ BaseResult result = new BaseResult(); result.setCode(ResultCode.RS500.getCode()); result.setMsg(ResultCode.RS500.getMsg()); return result; } public static &lt;T&gt; BaseResult&lt;T&gt; fail(String msg){ BaseResult result = new BaseResult(); result.setCode(ResultCode.RS500.getCode()); result.setMsg(msg); return result; } 3. 全局ResponseBody 处理全局处理之前 要确定哪些地方用到，会不会存在不使用的情况 不使用全局返回 定义注解 1234567891011mport java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target({ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)public @interface Primitive {} 需要借助SpringBoot 提供的ResponseBodyAdvice即可。 ResponseBodyAdvice 的作用：拦截Controller方法的返回值，统一处理返回值/响应体，一般用来统一返回格式，加解密，签名等等。 supports表示是否需要处理，那么这里我们通过接口方法上是否有上一步定义的Primitive注解来判断； beforeObjectWrite方法表示具体的处理，body表示原有的方法返回值，这里我们对其包裹一层，就实现了统一的格式返回。 123456789101112131415161718192021222324252627282930313233343536373839import com.fasterxml.jackson.databind.ObjectMapper;import lombok.SneakyThrows;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.core.MethodParameter;import org.springframework.http.MediaType;import org.springframework.http.converter.HttpMessageConverter;import org.springframework.http.server.ServerHttpRequest;import org.springframework.http.server.ServerHttpResponse;import org.springframework.web.bind.annotation.RestControllerAdvice;import org.springframework.web.servlet.mvc.method.annotation.ResponseBodyAdvice;import java.util.Objects;@RestControllerAdvice(basePackages = &quot;com.example.invoke&quot;)public class ResponseBodyWrapper implements ResponseBodyAdvice&lt;Object&gt; { @Autowired private ObjectMapper objectMapper; @Override public boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) { return Objects.requireNonNull(returnType.getMethod()).getAnnotation(Primitive.class) == null; } @SneakyThrows @Override public Object beforeBodyWrite(Object body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) { // 如果Controller直接返回String的话，SpringBoot是直接返回，故我们需要手动转换成json。备注：type：text/plain;charset=UTF-8 if(body instanceof String){ return objectMapper.writeValueAsString(BaseResult.success(body)); } // 如果对象已经是 BaseResult 直接返回即可 if (body instanceof BaseResult){ return body; } return BaseResult.success(body); }} TODO: 为什么 对于 STRING 类型 SpringBoot 是直接返回 ? spring 框架有个HttpMessageConverter类，就是专门用来处理流和接口的参数类型或返回值类型之间的转换的。方法也很好理解，就是判断能不能进行读操作，能的话就进行读操作；能不能进行写操作，能的话就进行写操作。 断点查看 发现这个converter是一个StringHttpMessageConverter类型，然后我们进入到addDefaultHeaders方法 代码执行到这里 发生 ClassCastException异常。 最后，我们终于找到了发生异常的原因，因为 260 的代码会执行getContentLength(t, headers.getContentType())这个方法，而这个方法会去执行StringHttpMessageConverter的getContentLength方法，如下图所示： 但是这个时候 t的类型已经被我们用beforeBodyWrite方法转为Result类型了，所以就发生了类型转换异常的错误。 建议使用这种方式 代替 手动转 json。这种 响应头是 application/json 12345678910111213141516171819import org.springframework.context.annotation.Configuration;import org.springframework.http.converter.HttpMessageConverter;import org.springframework.http.converter.StringHttpMessageConverter;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import java.util.List;@Configurationpublic class MyWebmvcConfiguration implements WebMvcConfigurer { @Override public void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { // 1. 将 json 处理的转换器放到第一位，使得先让 json 转换器处理返回值，这样 String转换器就处理不了了。 converters.add(0, new MappingJackson2HttpMessageConverter()); //2. 删除springboot默认的StringHttpMessageConverter解析器 // converters.removeIf(x -&gt; x instanceof StringHttpMessageConverter); }} 4. 全局异常处理器 @RestControllerAdvice，RestController的增强类，可用于实现全局异常处理器 @ExceptionHandler,统一处理某一类异常，从而减少代码重复率和复杂度，比如要获取自定义异常可以 @ExceptionHandler(BusinessException.class) @ResponseStatus指定客户端收到的http状态码 1234567891011@Slf4j@RestControllerAdvicepublic class ResultExceptionHandler { @ExceptionHandler(Exception.class) @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) public BaseResult&lt;String&gt; exception(Exception e){ log.error(&quot;全局异常信息 exception = {}&quot;,e.getMessage(),e); return BaseResult.fail(e.getMessage()); }} 编写 controller 1234567891011121314151617181920212223242526272829303132333435@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String sayHello(){ return &quot;123&quot;; } @GetMapping(&quot;/hello1&quot;) @Primitive public String sayHello2(){ return &quot;123&quot;; } @GetMapping(&quot;/hello2&quot;) public int sayHello22(){ return 1/0; } @GetMapping(&quot;/user&quot;) public User sayHello3(){ User user = new User(); user.setUsername(&quot;zhangsan&quot;); user.setPassword(&quot;123&quot;); return user; } @GetMapping(&quot;/user1&quot;) @Primitive public User sayHello4(){ User user = new User(); user.setUsername(&quot;zhangsan&quot;); user.setPassword(&quot;123&quot;); return user; }} 实际效果 1234567891011121314151617181920212223242526272829303132333435363738# hello{ &quot;code&quot;: 200, &quot;time&quot;: 1650780227306, &quot;msg&quot;: &quot;操作成功&quot;, &quot;body&quot;: &quot;123&quot;}# hello1123# hello2{ &quot;code&quot;: 500, &quot;time&quot;: 1650780285696, &quot;msg&quot;: &quot;/ by zero&quot;, &quot;body&quot;: null}# user{ &quot;code&quot;: 200, &quot;time&quot;: 1650778878610, &quot;msg&quot;: &quot;操作成功&quot;, &quot;body&quot;: { &quot;username&quot;: &quot;zhangsan&quot;, &quot;password&quot;: &quot;123&quot; }}# user1{ &quot;username&quot;: &quot;zhangsan&quot;, &quot;password&quot;: &quot;123&quot;} TODO : 返回的 Content-Type 没有指定编码格式 ? how to do ? 5. 全局 Response Type 设置How to Set a Header on a Response Adding a Header for All Responses 12345678910111213141516import org.springframework.stereotype.Component;import javax.servlet.*;import javax.servlet.http.HttpServletResponse;import java.io.IOException;@Componentpublic class AddResponseHeaderFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws IOException, ServletException { HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.addHeader(&quot;Content-Type&quot;,&quot;application/json;charset=UTF-8&quot;); httpServletResponse.addHeader(&quot;Content-TEST&quot;,&quot;123&quot;); filterChain.doFilter(request,response); }} 测试结果 官方文档地址 ：https://www.baeldung.com/spring-response-header","link":"/2022/04/21/springBoot/spring%20%E5%85%A8%E5%B1%80%E7%BB%9F%E4%B8%80%E7%BB%93%E6%9E%9C%E5%A4%84%E7%90%86/"},{"title":"spring拦截器介绍","text":"拦截器 spring boot 三种拦截 http 请求方式 Filter，interceptor 和 aop 拦截顺序是 : filter—&gt;Interceptor–&gt;ControllerAdvice–&gt;@Aspect –&gt;Controller； 区别： 过滤器Filter可以拿到原始的HTTP请求和响应的信息， 但是拿不到你真正处理请求方法的信息，也就是方法的信息。 拦截器Interceptor可以拿到原始的HTTP请求和响应的信息，也可以拿到你真正处理请求方法的信息，但是拿不到传进参数的那个值。 切片Aspect，既然Spring那么支持AOP，可以拿到原始的HTTP请求和响应的信息, 也可以拿到你真正处理请求方法的信息，也可以传进参数的那个值。 1. Filter实现 Filer 接口 12345678910111213141516171819202122232425262728293031323334353637/** * 自定义Filter * 对请求的header 过滤token * * 过滤器Filter可以拿到原始的HTTP请求和响应的信息， * 但是拿不到你真正处理请求方法的信息，也就是方法的信息 * * @Component 注解让拦截器注入Bean，从而让拦截器生效 * @WebFilter 配置拦截规则 * * 拦截顺序：filter—&gt;Interceptor--&gt;ControllerAdvice--&gt;@Aspect --&gt;Controller * */@Slf4j@Component@WebFilter(urlPatterns = {&quot;/**&quot;},filterName = &quot;tokenAuthorFilter&quot;)public class TokenFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { log.info(&quot;TokenFilter init {}&quot;,filterConfig.getFilterName()); } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { log.info(&quot;TokenFilter doFilter 我拦截到了请求&quot;);// log.info(&quot;TokenFilter doFilter&quot;,((HttpServletRequest)request).getHeader(&quot;token&quot;)); chain.doFilter(request,response);//到下一个链 } @Override public void destroy() { log.info(&quot;TokenFilter destroy&quot;); }} 2. Interceptor实现 HanderIntercetor 接口，然后配置进 Spring 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 自定义拦截器 * 自定义拦截器后，需要配置进Spring * * 拦截器Interceptor可以拿到原始的HTTP请求和响应的信息， * 也可以拿到你真正处理请求方法的信息，但是拿不到传进参数的那个值。 * *拦截顺序：filter—&gt;Interceptor--&gt;ControllerAdvice--&gt;@Aspect --&gt;Controller */@Slf4j@Componentpublic class TokenInterceptor implements HandlerInterceptor { /** * 在访问Controller某个方法之前这个方法会被调用。 * @param request * @param response * @param handler * @return false则表示不执行postHandle方法,true 表示执行postHandle方法 * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { log.info(&quot;Token Interceptor preHandle {}&quot;,&quot;&quot;); String token = request.getHeader(&quot;token&quot;); log.info(&quot;Token Interceptor preHandle token :{}&quot;,token); log.info(&quot;Token Interceptor preHandle uri {}&quot;,request.getRequestURL().toString()); //spring boot 2.0对静态资源也进行了拦截，当拦截器拦截到请求之后， // 但controller里并没有对应的请求时，该请求会被当成是对静态资源的请求。 // 此时的handler就是 ResourceHttpRequestHandler，就会抛出上述错误。 if (handler instanceof HandlerMethod){ HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); log.info(&quot;Token Interceptor preHandle getMethod {}&quot;,method.getName()); }else if(handler instanceof ResourceHttpRequestHandler){//静态资源 ResourceHttpRequestHandler resourceHttpRequestHandler = (ResourceHttpRequestHandler) handler; log.info(&quot;Token Interceptor preHandle getMethod {}&quot;,resourceHttpRequestHandler.getMediaTypes()); } //false则表示不执行postHandle方法,不执行下一步chain链，直接返回response return true; } /** * 请求处理之后进行调用，但是在视图被渲染之前（Controller方法调用之后） * preHandle方法处理之后这个方法会被调用，如果控制器Controller出现了异常，则不会执行此方法 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { log.info(&quot;Token Interceptor postHandle&quot;); } /** * 不管有没有异常，这个afterCompletion都会被调用 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { log.info(&quot;Token Interceptor afterCompletion&quot;); }} 3. aop 拦截添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; aop 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * pom.xml 添加Aop支持 * &lt;dependency&gt; * &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; * &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; * &lt;/dependency&gt; * * 切片Aspect，既然Spring那么支持AOP，可以拿到原始的HTTP请求和响应的信息， * 也可以拿到你真正处理请求方法的信息，也可以传进参数的那个值。 * * 拦截顺序：filter—&gt;Interceptor--&gt;ControllerAdvice--&gt;@Aspect --&gt;Controller */@Slf4j@Component //表示它是一个Spring的组件@Aspect //表示它是一个切面public class HttpAspect { /** * 通过ProceedingJoinPoint对象的getArgs()我们可以得到传进来的参数。 * 通过ProceedingJoinPoint对象的proceed()我们可以得到拿到切面方法返回值的对象。 * @param pjp * @return * 环绕通知 首先是:包名 然后是: 类名 然后是方法名:方法名 括号内是:参数 */ @Around(&quot;execution(* com.example.java.controller.*.*(..))&quot;) public Object handleControllerMethod(ProceedingJoinPoint pjp) throws Throwable { log.info(&quot;HttpAspect handleControllerMethod filter start&quot;); //原始的HTTP请求和响应的信息 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); HttpServletResponse response = attributes.getResponse(); Signature signature = pjp.getSignature(); MethodSignature methodSignature = (MethodSignature)signature; //获取当前执行的方法 Method targetMethod = methodSignature.getMethod(); log.info(&quot;当前执行的方法:{}&quot;,targetMethod.getName()); //获取参数 Object[] objs = pjp.getArgs(); for (Object obj:objs){ log.info(&quot;参数:&quot;+obj); } //获取返回对象 Object object = pjp.proceed(); log.info(&quot;获得返回对象 :{}&quot;,object); log.info(&quot;HttpAspect handleControllerMethod filter end&quot;); return pjp.proceed();//代理方法的返回值 }}","link":"/2022/04/23/springBoot/%E8%BF%87%E6%BB%A4%E5%99%A8,%E6%8B%A6%E6%88%AA%E5%99%A8,AOP%E6%AF%94%E8%BE%83/"},{"title":"链表","text":"链表链表的节点定义1234567891011121314151617public class ListNode { int value; ListNode next; public ListNode() { } public ListNode(int value) { this.value = value; next = null; } public ListNode(int value, ListNode next) { this.value = value; this.next = next; }} 链表的定义：添加元素,反转方法,打印12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class LinkList { ListNode head; // 尾插法 public void endAdd(int n) { ListNode listnode = new ListNode(n); if(head==null) { head=listnode; return; } ListNode temp = head; while(temp.next!=null) { temp=temp.next; } temp.next=listnode; } // 头插法 public void startAdd(int n) { ListNode listnode = new ListNode(n); listnode.next = head; head = listnode; } //把添加的值打印的方法 public void printLink() { ListNode temp=head; while(temp!=null) { System.out.print(temp.value+&quot; &quot;); temp=temp.next; } System.out.println(); } public void reverse(){ //定义一个带头节点的 ListNode resultList = new ListNode(-1); //循环节点 ListNode p = head; while(p!= null){ //保存插入点之后的数据 ListNode tempList = p.next; p.next = resultList.next; resultList.next = p; p = tempList; } head = resultList.next; }} 反转原理：遍历原有链表，通过一个临时 节点，交换原始链表的第一个节点，赋值给 新的链表尾部 测试结果123456789101112131415 public static void main(String[] args) { LinkList linkList = new LinkList(); linkList.startAdd(1); linkList.startAdd(2); linkList.startAdd(3); linkList.startAdd(4); linkList.startAdd(5); linkList.startAdd(6); linkList.printLink(); linkList.reverse(); linkList.printLink(); }# 控制台输出6 5 4 3 2 1 1 2 3 4 5 6","link":"/2022/04/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"},{"title":"webservice","text":"HttpClient 调用 webservice 服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697 /** 具体的业务请求 */ public HISResult hisWebBusiness(HIS_Log log,String wsURL, String tranCode, String inXml) throws Exception { HISResult hr = new HISResult(); try { // time1 long time1 = System.currentTimeMillis(); log.setInXML(inXml); log.setTranCode(tranCode); StringBuffer xMLcontent = new StringBuffer(); xMLcontent.append(&quot;&lt;soapenv:Envelope xmlns:soapenv=\\&quot;http://schemas.xmlsoap.org/soap/envelope/\\&quot; xmlns:zls=\\&quot;ZLSoft\\&quot;&gt;\\n&quot;); xMLcontent.append(&quot;&lt;soapenv:Header/&gt;\\n&quot;); xMLcontent.append(&quot;&lt;soapenv:Body&gt;\\n&quot;); xMLcontent.append(&quot;&lt;zls:fRun&gt;\\n&quot;); xMLcontent.append(&quot;&lt;zls:TranCode&gt;&quot;); xMLcontent.append(tranCode); xMLcontent.append(&quot;&lt;/zls:TranCode&gt;\\n&quot;); xMLcontent.append(&quot;&lt;zls:InXml&gt;\\n&quot;); xMLcontent.append(&quot;&lt;![CDATA[&quot;); xMLcontent.append(inXml); xMLcontent.append(&quot;]]&gt;&quot;); xMLcontent.append(&quot;&lt;/zls:InXml&gt;\\n&quot;); xMLcontent.append(&quot;&lt;zls:OutXml&gt;\\n&quot;); xMLcontent.append(&quot;&lt;/zls:OutXml&gt;\\n&quot;); xMLcontent.append(&quot;&lt;/zls:fRun&gt;\\n&quot;); xMLcontent.append(&quot;&lt;/soapenv:Body&gt;\\n&quot;); xMLcontent.append(&quot;&lt;/soapenv:Envelope&gt;&quot;); String message = HTTPClientTypeXml(wsURL, xMLcontent.toString(), &quot;UTF-8&quot;); String outxml = getResultNode(message, &quot;OutXml&quot;); String xml = StringEscapeUtils.unescapeHtml(outxml); if (xml != null) { // 去除 CDATA xml = xml.replace(&quot;&lt;![CDATA[&quot;, &quot;&quot;); xml = xml.replace(&quot;]]&gt;&quot;, &quot;&quot;); // 去除前言 xml = xml.replace(&quot;&lt;?xml version=\\&quot;1.0\\&quot; encoding=\\&quot;UTF-8\\&quot;?&gt;&quot;,&quot;&quot;); // 去除空格 xml = xml.trim(); } log.setOutXML(xml); hr.setOutXml(xml); long time2 = System.currentTimeMillis(); long time3 = time2 - time1; hr.setJksj((int) time3); } catch (Exception e) { log.setOutXML(e.getMessage()); throw e; }finally{ logDao.saveHI_LOG(log); } return hr;}/** 对 xml 出参截取工具 */ private String getResultNode(String responseMsg,String node){ if (responseMsg == null &amp;&amp; &quot;&quot;.equals(responseMsg)) { return responseMsg; } responseMsg = responseMsg.replaceAll(&quot;&amp;lt;&quot;, &quot;&lt;&quot;); responseMsg = responseMsg.replaceAll(&quot;&amp;gt;&quot;, &quot;&gt;&quot;); responseMsg = responseMsg.replaceAll(&quot;&gt; &lt;&quot;, &quot;&gt;&lt;&quot;); responseMsg = responseMsg.replaceAll(&quot;\\n&quot;, &quot;&quot;); responseMsg = responseMsg.replaceAll(&quot;\\r&quot;, &quot;&quot;); String begin = &quot;&lt;&quot;+node+&quot;&gt;&quot;; String end = &quot;&lt;/&quot;+node+&quot;&gt;&quot;; String regex = begin + &quot;(.*)&quot; + end; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(responseMsg); if (m.find()) { responseMsg = m.group(1); } return responseMsg;} /** HTTP 调用方法工具 */ private String HTTPClientTypeXml(String wsurl,String request,String charset) throws Exception{ CloseableHttpClient httpClient = httpBulder.build(); HttpPost post = new HttpPost(wsurl); post.setHeader(&quot;Content-type&quot;, &quot;text/xml; charset=&quot;+charset); post.setConfig(requestConfig); StringEntity entity = new StringEntity(request, Charset.forName(charset)); entity.setContentEncoding(charset); // 发送xml格式的数据请求 entity.setContentType(&quot;text/xml&quot;); post.setEntity(entity); HttpResponse response = httpClient.execute(post); HttpEntity entity2 = response.getEntity(); String message = null; message = EntityUtils.toString(entity2, charset); return message;}","link":"/2022/05/08/%E5%B7%A5%E5%85%B7%E7%B1%BB/HttpClient%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"OkHttp3 Utils","text":"添加依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.10.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.60&lt;/version&gt;&lt;/dependency&gt; 引入json是因为工具类中有些地方用到了，现在通信都流行使用json传输，也少不了要这个jar包 工具类代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293import com.alibaba.fastjson.JSON;import okhttp3.*; import javax.net.ssl.SSLContext;import javax.net.ssl.SSLSocketFactory;import javax.net.ssl.TrustManager;import javax.net.ssl.X509TrustManager;import java.io.IOException;import java.net.URLEncoder;import java.security.SecureRandom;import java.security.cert.X509Certificate;import java.util.LinkedHashMap;import java.util.Map;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit; public class OkHttpUtils { private static volatile OkHttpClient okHttpClient = null; private static volatile Semaphore semaphore = null; private Map&lt;String, String&gt; headerMap; private Map&lt;String, String&gt; paramMap; private String url; private Request.Builder request; /** * 初始化okHttpClient，并且允许https访问 */ private OkHttpUtils() { if (okHttpClient == null) { synchronized (OkHttpUtils.class) { if (okHttpClient == null) { TrustManager[] trustManagers = buildTrustManagers(); okHttpClient = new OkHttpClient.Builder() .connectTimeout(15, TimeUnit.SECONDS) .writeTimeout(20, TimeUnit.SECONDS) .readTimeout(20, TimeUnit.SECONDS) .sslSocketFactory(createSSLSocketFactory(trustManagers), (X509TrustManager) trustManagers[0]) .hostnameVerifier((hostName, session) -&gt; true) .retryOnConnectionFailure(true) .build(); addHeader(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&quot;); } } } } /** * 用于异步请求时，控制访问线程数，返回结果 * * @return */ private static Semaphore getSemaphoreInstance() { //只能1个线程同时访问 synchronized (OkHttpUtils.class) { if (semaphore == null) { semaphore = new Semaphore(0); } } return semaphore; } /** * 创建OkHttpUtils * * @return */ public static OkHttpUtils builder() { return new OkHttpUtils(); } /** * 添加url * * @param url * @return */ public OkHttpUtils url(String url) { this.url = url; return this; } /** * 添加参数 * * @param key 参数名 * @param value 参数值 * @return */ public OkHttpUtils addParam(String key, String value) { if (paramMap == null) { paramMap = new LinkedHashMap&lt;&gt;(16); } paramMap.put(key, value); return this; } /** * 添加请求头 * * @param key 参数名 * @param value 参数值 * @return */ public OkHttpUtils addHeader(String key, String value) { if (headerMap == null) { headerMap = new LinkedHashMap&lt;&gt;(16); } headerMap.put(key, value); return this; } /** * 初始化get方法 * * @return */ public OkHttpUtils get() { request = new Request.Builder().get(); StringBuilder urlBuilder = new StringBuilder(url); if (paramMap != null) { urlBuilder.append(&quot;?&quot;); try { for (Map.Entry&lt;String, String&gt; entry : paramMap.entrySet()) { urlBuilder.append(URLEncoder.encode(entry.getKey(), &quot;utf-8&quot;)). append(&quot;=&quot;). append(URLEncoder.encode(entry.getValue(), &quot;utf-8&quot;)). append(&quot;&amp;&quot;); } } catch (Exception e) { e.printStackTrace(); } urlBuilder.deleteCharAt(urlBuilder.length() - 1); } request.url(urlBuilder.toString()); return this; } /** * 初始化post方法 * * @param isJsonPost true等于json的方式提交数据，类似postman里post方法的raw * false等于普通的表单提交 * @return */ public OkHttpUtils post(boolean isJsonPost) { RequestBody requestBody; if (isJsonPost) { String json = &quot;&quot;; if (paramMap != null) { json = JSON.toJSONString(paramMap); } requestBody = RequestBody.create(MediaType.parse(&quot;application/json; charset=utf-8&quot;), json); } else { FormBody.Builder formBody = new FormBody.Builder(); if (paramMap != null) { paramMap.forEach(formBody::add); } requestBody = formBody.build(); } request = new Request.Builder().post(requestBody).url(url); return this; } /** * 同步请求 * * @return */ public String sync() { setHeader(request); try { Response response = okHttpClient.newCall(request.build()).execute(); assert response.body() != null; return response.body().string(); } catch (IOException e) { e.printStackTrace(); return &quot;请求失败：&quot; + e.getMessage(); } } /** * 异步请求，有返回值 */ public String async() { StringBuilder buffer = new StringBuilder(&quot;&quot;); setHeader(request); okHttpClient.newCall(request.build()).enqueue(new Callback() { @Override public void onFailure(Call call, IOException e) { buffer.append(&quot;请求出错：&quot;).append(e.getMessage()); } @Override public void onResponse(Call call, Response response) throws IOException { assert response.body() != null; buffer.append(response.body().string()); getSemaphoreInstance().release(); } }); try { getSemaphoreInstance().acquire(); } catch (InterruptedException e) { e.printStackTrace(); } return buffer.toString(); } /** * 异步请求，带有接口回调 * * @param callBack */ public void async(ICallBack callBack) { setHeader(request); okHttpClient.newCall(request.build()).enqueue(new Callback() { @Override public void onFailure(Call call, IOException e) { callBack.onFailure(call, e.getMessage()); } @Override public void onResponse(Call call, Response response) throws IOException { assert response.body() != null; callBack.onSuccessful(call, response.body().string()); } }); } /** * 为request添加请求头 * * @param request */ private void setHeader(Request.Builder request) { if (headerMap != null) { try { for (Map.Entry&lt;String, String&gt; entry : headerMap.entrySet()) { request.addHeader(entry.getKey(), entry.getValue()); } } catch (Exception e) { e.printStackTrace(); } } } /** * 生成安全套接字工厂，用于https请求的证书跳过 * * @return */ private static SSLSocketFactory createSSLSocketFactory(TrustManager[] trustAllCerts) { SSLSocketFactory ssfFactory = null; try { SSLContext sc = SSLContext.getInstance(&quot;SSL&quot;); sc.init(null, trustAllCerts, new SecureRandom()); ssfFactory = sc.getSocketFactory(); } catch (Exception e) { e.printStackTrace(); } return ssfFactory; } private static TrustManager[] buildTrustManagers() { return new TrustManager[]{ new X509TrustManager() { @Override public void checkClientTrusted(X509Certificate[] chain, String authType) { } @Override public void checkServerTrusted(X509Certificate[] chain, String authType) { } @Override public X509Certificate[] getAcceptedIssuers() { return new X509Certificate[]{}; } } }; } /** * 自定义一个接口回调 */ public interface ICallBack { void onSuccessful(Call call, String data); void onFailure(Call call, String errorMsg); }} 调用示例 123456789101112131415161718192021222324252627282930313233343536373839404142public static void main(String[] args) { // get请求，方法顺序按照这种方式，切记选择post/get一定要放在倒数第二，同步或者异步倒数第一，才会正确执行 OkHttpUtils.builder().url(&quot;请求地址，http/https都可以&quot;) // 有参数的话添加参数，可多个 .addParam(&quot;参数名&quot;, &quot;参数值&quot;) .addParam(&quot;参数名&quot;, &quot;参数值&quot;) // 也可以添加多个 .addHeader(&quot;Content-Type&quot;, &quot;application/json; charset=utf-8&quot;) .get() // 可选择是同步请求还是异步请求 //.async(); .sync(); // post请求，分为两种，一种是普通表单提交，一种是json提交 OkHttpUtils.builder().url(&quot;请求地址，http/https都可以&quot;) // 有参数的话添加参数，可多个 .addParam(&quot;参数名&quot;, &quot;参数值&quot;) .addParam(&quot;参数名&quot;, &quot;参数值&quot;) // 也可以添加多个 .addHeader(&quot;Content-Type&quot;, &quot;application/json; charset=utf-8&quot;) // 如果是true的话，会类似于postman中post提交方式的raw，用json的方式提交，不是表单 // 如果是false的话传统的表单提交 .post(true) .sync(); // 选择异步有两个方法，一个是带回调接口，一个是直接返回结果 OkHttpUtils.builder().url(&quot;&quot;) .post(false) .async(); OkHttpUtils.builder().url(&quot;&quot;).post(false).async(new OkHttpUtils.ICallBack() { @Override public void onSuccessful(Call call, String data) { // 请求成功后的处理 } @Override public void onFailure(Call call, String errorMsg) { // 请求失败后的处理 } });}","link":"/2022/05/15/%E5%B7%A5%E5%85%B7%E7%B1%BB/OkHttp3%20%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"IpUtils","text":"获取真实的 ip 地址 1234567891011121314151617181920212223242526272829303132333435363738394041424344import lombok.extern.slf4j.Slf4j;import org.springframework.util.StringUtils;import javax.servlet.http.HttpServletRequest;@Slf4jpublic class IpUtils { /** * 获取IP地址 * 使用Nginx等反向代理软件， 则不能通过request.getRemoteAddr()获取IP地址 * 如果使用了多级反向代理的话，X-Forwarded-For的值并不止一个，而是一串IP地址，X-Forwarded-For中第一个非unknown的有效IP字符串，则为真实IP地址 */ public static String getIpAddr(HttpServletRequest request) { String ip = null; try { ip = request.getHeader(&quot;x-forwarded-for&quot;); if (StringUtils.isEmpty(ip) || &quot;unknown&quot;.equalsIgnoreCase(ip)) { ip = request.getHeader(&quot;Proxy-Client-IP&quot;); } if (StringUtils.isEmpty(ip) || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) { ip = request.getHeader(&quot;WL-Proxy-Client-IP&quot;); } if (StringUtils.isEmpty(ip) || &quot;unknown&quot;.equalsIgnoreCase(ip)) { ip = request.getHeader(&quot;HTTP_CLIENT_IP&quot;); } if (StringUtils.isEmpty(ip) || &quot;unknown&quot;.equalsIgnoreCase(ip)) { ip = request.getHeader(&quot;HTTP_X_FORWARDED_FOR&quot;); } if (StringUtils.isEmpty(ip) || &quot;unknown&quot;.equalsIgnoreCase(ip)) { ip = request.getRemoteAddr(); } } catch (Exception e) { log.error(&quot;IPUtils ERROR &quot;, e); } // 使用代理，则获取第一个IP地址 if (StringUtils.isEmpty(ip) &amp;&amp; ip.length() &gt; 15) { if (ip.indexOf(&quot;,&quot;) &gt; 0) { ip = ip.substring(0, ip.indexOf(&quot;,&quot;)); } } return ip; }}","link":"/2022/05/18/%E5%B7%A5%E5%85%B7%E7%B1%BB/IpUtils/"},{"title":"RestTmplate","text":"RestTmplate 介绍RestTmplate 是从 sprng 3.0 开始 支持的 http 请求工具。提供常见 REST 请求方法模板：GET POST PUT DELETERestTmplate 实现了 RestOperation 接口，RestOperation定义了 RESTFUL 操作 声明式使用12345678@BeanRestTemplate restTemplateOne(){ return new RestTemplate();}@AutowiredRestTemplate restTemplate; GETgetForObject 返回的是一个对象，对象是返回的具体值。3种请求方式 getForEntity 返回是一个 ResponseEntity 1234567891011//1String s = restTemplateOne.getForObject(&quot;http://provider/hello?name={1}&quot;,String.class,&quot;test&quot;);//2Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();map.put(&quot;name&quot;,&quot;test&quot;);String s1 = restTemplateOne.getForObject(&quot;http://provider/hello?name={name}&quot;,String.class,map);//3String url = &quot;http://provider/hello?name=&quot; + &quot;test&quot;;URI url1 = URI.create(url);String s2 = restTemplateOne.getForObject(url1,String.class); POST123456789101112131415161718@GetMapping(&quot;/hello6&quot;) public String hello6(){ //1 key vaues 传递参数 MultiValueMap&lt;String,Object&gt; map = new LinkedMultiValueMap&lt;String,Object&gt;(); map.add(&quot;id&quot;,123); map.add(&quot;username&quot;,&quot;java&quot;); map.add(&quot;password&quot;,&quot;123&quot;); String url = &quot;http://provider/user1&quot;; User user = restTemplateTwo.postForObject(url,map,User.class); System.out.println(user); user.setId(456); String url2 = &quot;http://provider/user2&quot;; //2 json 传递参数 User user1 = restTemplateTwo.postForObject(url2,user,User.class); System.out.println(user1); return &quot;ok!!!&quot;; }","link":"/2022/05/10/%E5%B7%A5%E5%85%B7%E7%B1%BB/RestTemplate%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"文件资源IO流","text":"1. FileCopyUtils 文件 1.1 输入123456// 从文件中读入到字节数组中byte[] copyToByteArray(File in)// 从输入流中读入到字节数组中byte[] copyToByteArray(InputStream in)// 从输入流中读入到字符串中String copyToString(Reader in) 1.2 输出123456789101112// 从字节数组到文件void copy(byte[] in, File out)// 从文件到文件int copy(File in, File out)// 从字节数组到输出流void copy(byte[] in, OutputStream out)// 从输入流到输出流int copy(InputStream in, OutputStream out)// 从输入流到输出流int copy(Reader in, Writer out)// 从字符串到输出流void copy(String in, Writer out) 2. ResourceUtils 资源 1. 从资源路径获取文件123456// 判断字符串是否是一个合法的 URL 字符串。static boolean isUrl(String resourceLocation)// 获取 URLstatic URL getURL(String resourceLocation)// 获取文件（在 JAR 包内无法正常使用，需要是一个独立的文件）static File getFile(String resourceLocation) 2.2 Resource1234567891011121314151617181920// 文件系统资源 D:\\...FileSystemResource// URL 资源，如 file://... http://...UrlResource// 类路径下的资源，classpth:...ClassPathResource// Web 容器上下文中的资源（jar 包、war 包）ServletContextResource// 判断资源是否存在boolean exists()// 从资源中获得 File 对象File getFile()// 从资源中获得 URI 对象URI getURI()// 从资源中获得 URI 对象URL getURL()// 获得资源的 InputStreamInputStream getInputStream()// 获得资源的描述信息String getDescription() 3. StreamUtils IO流 3.1 输入1234void copy(byte[] in, OutputStream out)int copy(InputStream in, OutputStream out)void copy(String in, Charset charset, OutputStream out)long copyRange(InputStream in, OutputStream out, long start, long end) 3.2 输出1234byte[] copyToByteArray(InputStream in)String copyToString(InputStream in, Charset charset)// 舍弃输入流中的内容int drain(InputStream in) 4. ReflectionUtils 反射 4.1 获取方法123456789101112131415161718// 在类中查找指定方法Method findMethod(Class&lt;?&gt; clazz, String name)// 同上，额外提供方法参数类型作查找条件Method findMethod(Class&lt;?&gt; clazz, String name, Class&lt;?&gt;... paramTypes)// 获得类中所有方法，包括继承而来的Method[] getAllDeclaredMethods(Class&lt;?&gt; leafClass)// 在类中查找指定构造方法Constructor&lt;T&gt; accessibleConstructor(Class&lt;T&gt; clazz, Class&lt;?&gt;... parameterTypes)// 是否是 equals() 方法boolean isEqualsMethod(Method method)// 是否是 hashCode() 方法boolean isHashCodeMethod(Method method)// 是否是 toString() 方法boolean isToStringMethod(Method method)// 是否是从 Object 类继承而来的方法boolean isObjectMethod(Method method)// 检查一个方法是否声明抛出指定异常boolean declaresException(Method method, Class&lt;?&gt; exceptionType) 4.2 执行方法12345678// 执行方法Object invokeMethod(Method method, Object target)// 同上，提供方法参数Object invokeMethod(Method method, Object target, Object... args)// 取消 Java 权限检查。以便后续执行该私有方法void makeAccessible(Method method)// 取消 Java 权限检查。以便后续执行私有构造方法void makeAccessible(Constructor&lt;?&gt; ctor) 4.3 获取字段123456// 在类中查找指定属性Field findField(Class&lt;?&gt; clazz, String name)// 同上，多提供了属性的类型Field findField(Class&lt;?&gt; clazz, String name, Class&lt;?&gt; type)// 是否为一个 &quot;public static final&quot; 属性boolean isPublicStaticFinal(Field field) 4.4 设置字段123456789101112131415// 获取 target 对象的 field 属性值Object getField(Field field, Object target)// 设置 target 对象的 field 属性值，值为 valuevoid setField(Field field, Object target, Object value)// 同类对象属性对等赋值void shallowCopyFieldState(Object src, Object dest)// 取消 Java 的权限控制检查。以便后续读写该私有属性void makeAccessible(Field field)// 对类的每个属性执行 callbackvoid doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc)// 同上，多了个属性过滤功能。void doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc, ReflectionUtils.FieldFilter ff)// 同上，但不包括继承而来的属性void doWithLocalFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc) 5. AopUtils 5.1 判断代理类型123456// 判断是不是 Spring 代理对象boolean isAopProxy()// 判断是不是 jdk 动态代理对象isJdkDynamicProxy()// 判断是不是 CGLIB 代理对象boolean isCglibProxy() 5.2 获取被代理对象的 class12// 获取被代理的目标 classClass&lt;?&gt; getTargetClass()","link":"/2022/05/21/%E5%B7%A5%E5%85%B7%E7%B1%BB/%E5%B7%A5%E5%85%B7%E7%B1%BB%E9%9B%86%E5%90%88-%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90%E6%B5%81/"},{"title":"字符对象集合","text":"1. Assert 断言 123456789101112131415161718// 要求参数 object 必须为非空（Not Null），否则抛出异常，不予放行// 参数 message 参数用于定制异常信息。void notNull(Object object, String message)// 要求参数必须空（Null），否则抛出异常，不予『放行』。// 和 notNull() 方法断言规则相反void isNull(Object object, String message)// 要求参数必须为真（True），否则抛出异常，不予『放行』。void isTrue(boolean expression, String message)// 要求参数（List/Set）必须非空（Not Empty），否则抛出异常，不予放行void notEmpty(Collection collection, String message)// 要求参数（String）必须有长度（即，Not Empty），否则抛出异常，不予放行void hasLength(String text, String message)// 要求参数（String）必须有内容（即，Not Blank），否则抛出异常，不予放行void hasText(String text, String message)// 要求参数是指定类型的实例，否则抛出异常，不予放行void isInstanceOf(Class type, Object obj, String message)// 要求参数 `subType` 必须是参数 superType 的子类或实现类，否则抛出异常，不予放行void isAssignable(Class superType, Class subType, String message) 2. ObjectUtils 对象 2.1 获取对象的基本信息123456789101112// 获取对象的类名。参数为 null 时，返回字符串：&quot;null&quot;String nullSafeClassName(Object obj)// 参数为 null 时，返回 0int nullSafeHashCode(Object object)// 参数为 null 时，返回字符串：&quot;null&quot;String nullSafeToString(boolean[] array)// 获取对象 HashCode（十六进制形式字符串）。参数为 null 时，返回 0String getIdentityHexString(Object obj)// 获取对象的类名和 HashCode。参数为 null 时，返回字符串：&quot;&quot;String identityToString(Object obj)// 相当于 toString()方法，但参数为 null 时，返回字符串：&quot;&quot;String getDisplayString(Object obj) 2.2 判断工具1234567891011121314151617// 判断数组是否为空boolean isEmpty(Object[] array)// 判断参数对象是否是数组boolean isArray(Object obj)// 判断数组中是否包含指定元素boolean containsElement(Object[] array, Object element)// 相等，或同为 null时，返回 trueboolean nullSafeEquals(Object o1, Object o2)/*判断参数对象是否为空，判断标准为： Optional: Optional.empty() Array: length == 0CharSequence: length == 0 Collection: Collection.isEmpty() Map: Map.isEmpty() */boolean isEmpty(Object obj) 2.3 其他1234// 向参数数组的末尾追加新元素，并返回一个新数组&lt;A, O extends A&gt; A[] addObjectToArray(A[] array, O obj)// 原生基础类型数组 --&gt; 包装类数组Object[] toObjectArray(Object source) 3. StringUtils 字符串工具 3.1 判断工具12345678910111213141516// 判断字符串是否为 null，或 &quot;&quot;。注意，包含空白符的字符串为非空boolean isEmpty(Object str)// 判断字符串是否是以指定内容结束。忽略大小写boolean endsWithIgnoreCase(String str, String suffix)// 判断字符串是否已指定内容开头。忽略大小写boolean startsWithIgnoreCase(String str, String prefix)// 是否包含空白符boolean containsWhitespace(String str)// 判断字符串非空且长度不为 0，即，Not Emptyboolean hasLength(CharSequence str)// 判断字符串是否包含实际内容，即非仅包含空白符，也就是 Not Blankboolean hasText(CharSequence str)// 判断字符串指定索引处是否包含一个子串。boolean substringMatch(CharSequence str, int index, CharSequence substring)// 计算一个字符串中指定子串的出现次数int countOccurrencesOf(String str, String sub) 3.2 操作工具12345678910111213141516171819202122// 查找并替换指定子串String replace(String inString, String oldPattern, String newPattern)// 去除尾部的特定字符String trimTrailingCharacter(String str, char trailingCharacter)// 去除头部的特定字符String trimLeadingCharacter(String str, char leadingCharacter)// 去除头部的空白符String trimLeadingWhitespace(String str)// 去除头部的空白符String trimTrailingWhitespace(String str)// 去除头部和尾部的空白符String trimWhitespace(String str)// 删除开头、结尾和中间的空白符String trimAllWhitespace(String str)// 删除指定子串String delete(String inString, String pattern)// 删除指定字符（可以是多个）String deleteAny(String inString, String charsToDelete)// 对数组的每一项执行 trim() 方法String[] trimArrayElements(String[] array)// 将 URL 字符串进行解码String uriDecode(String source, Charset charset) 3.3 路径相关方法1234567891011121314// 解析路径字符串，优化其中的 “..”String cleanPath(String path)// 解析路径字符串，解析出文件名部分String getFilename(String path)// 解析路径字符串，解析出文件后缀名String getFilenameExtension(String path)// 比较两个两个字符串，判断是否是同一个路径。会自动处理路径中的 “..”boolean pathEquals(String path1, String path2)// 删除文件路径名中的后缀部分String stripFilenameExtension(String path)// 以 “. 作为分隔符，获取其最后一部分String unqualify(String qualifiedName)// 以指定字符作为分隔符，获取其最后一部分String unqualify(String qualifiedName, char separator) 4. CollectionUtils 集合判断工具 4.1 集合判断工具123456789101112// 判断 List/Set 是否为空boolean isEmpty(Collection&lt;?&gt; collection)// 判断 Map 是否为空boolean isEmpty(Map&lt;?,?&gt; map)// 判断 List/Set 中是否包含某个对象boolean containsInstance(Collection&lt;?&gt; collection, Object element)// 以迭代器的方式，判断 List/Set 中是否包含某个对象boolean contains(Iterator&lt;?&gt; iterator, Object element)// 判断 List/Set 是否包含某些对象中的任意一个boolean containsAny(Collection&lt;?&gt; source, Collection&lt;?&gt; candidates)// 判断 List/Set 中的每个元素是否唯一。即 List/Set 中不存在重复元素boolean hasUniqueObject(Collection&lt;?&gt; collection) 4.2 集合操作工具12345678910111213141516// 将 Array 中的元素都添加到 List/Set 中&lt;E&gt; void mergeArrayIntoCollection(Object array, Collection&lt;E&gt; collection)// 将 Properties 中的键值对都添加到 Map 中&lt;K,V&gt; void mergePropertiesIntoMap(Properties props, Map&lt;K,V&gt; map)// 返回 List 中最后一个元素&lt;T&gt; T lastElement(List&lt;T&gt; list)// 返回 Set 中最后一个元素&lt;T&gt; T lastElement(Set&lt;T&gt; set)// 返回参数 candidates 中第一个存在于参数 source 中的元素&lt;E&gt; E findFirstMatch(Collection&lt;?&gt; source, Collection&lt;E&gt; candidates)// 返回 List/Set 中指定类型的元素。&lt;T&gt; T findValueOfType(Collection&lt;?&gt; collection, Class&lt;T&gt; type)// 返回 List/Set 中指定类型的元素。如果第一种类型未找到，则查找第二种类型，以此类推Object findValueOfType(Collection&lt;?&gt; collection, Class&lt;?&gt;[] types)// 返回 List/Set 中元素的类型Class&lt;?&gt; findCommonElementType(Collection&lt;?&gt; collection)","link":"/2022/05/21/%E5%B7%A5%E5%85%B7%E7%B1%BB/%E5%B7%A5%E5%85%B7%E7%B1%BB%E9%9B%86%E5%90%88-%E5%AD%97%E7%AC%A6%E5%AF%B9%E8%B1%A1%E9%9B%86%E5%90%88/"},{"title":"SpringUtil","text":"SpringBoot开发环境中，在普通类或其他工具类中需要获取容器中的对象在开发中是较为常见的现象，可通过如下方式创建SpringUtil 工具类 123456789101112131415161718192021222324252627282930313233343536373839404142import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.stereotype.Component;@Componentpublic class SpringUtil implements ApplicationContextAware { private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { if (SpringUtil.applicationContext == null){ SpringUtil.applicationContext = applicationContext; } } //获取applicationContext public static ApplicationContext getApplicationContext() { return applicationContext; } //通过name获取 Bean. public static Object getBean(String name){ return getApplicationContext().getBean(name); } //通过class获取Bean. public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz){ return getApplicationContext().getBean(clazz); } //通过name,以及Clazz返回指定的Bean public static &lt;T&gt; T getBean(String name,Class&lt;T&gt; clazz){ return getApplicationContext().getBean(name, clazz); }}","link":"/2022/05/11/%E5%B7%A5%E5%85%B7%E7%B1%BB/SpringUtil/"},{"title":"AOP实现限流器","text":"1. 准备工作 创建一个 spring boot 项目，引入 Web 和 Redis 依赖，考虑接口限流通过注解来标记，注解是通过 AOP 来解析的，所 以还需要加上 AOP 依赖 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 提前准备好 Redis 示例，在项目配置好后，配置 redis 基本信息 123spring.redis.host=localhostspring.redis.port=6379spring.redis.password=123 2. 限流注解 限流一般分为 2种 情况： 针对接口全局性限流，例如接口1分钟可以访问 100次 针对某一个 ip 地址的限流，例如某个 ip 地址可以在 1分钟 访问 100次 创建枚举类 12345678910public enum LimitType { /** * 默认策略全局限流 */ DEFAULT, /** * 根据请求者IP进行限流 */ IP} 创建限流注解 123456789101112131415161718192021222324@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface RateLimiter { /** * 限流key */ String key() default &quot;rate_limit:&quot;; /** * 限流时间,单位秒 */ int time() default 60; /** * 限流次数 */ int count() default 100; /** * 限流类型 */ LimitType limitType() default LimitType.DEFAULT;} 第一个参数限流的 key，这个仅仅是一个前缀，将来完整的 key 是这个前缀再加上接口方法的完整路径，共同组成限流 key，这个 key 将被存入到 Redis 中。 将来哪个接口需要限流，就在哪个接口上添加 @RateLimiter 注解，然后配置相关参数即可。 3. 定制 RedisTemplate 在 Spring Boot 中，我们其实更习惯使用 Spring Data Redis 来操作 Redis，不过默认的 RedisTemplate 有一个小坑，就是序列化用的是 JdkSerializationRedisSerializer，不知道小伙伴们有没有注意过，直接用这个序列化工具将来存到 Redis 上的 key 和 value 都会莫名其妙多一些前缀，这就导致你用命令读取的时候可能会出错。 可能有小伙伴会说为什么不用 StringRedisTemplate 呢？StringRedisTemplate 确实不存在上面所说的问题，但是它能够存储的数据类型不够丰富，所以这里不考虑。 修改 RedisTemplate 序列化方案 1234567891011121314151617181920@Configurationpublic class RedisConfig { @Bean public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory connectionFactory) { RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(connectionFactory); // 使用Jackson2JsonRedisSerialize 替换默认序列化(默认采用的是JDK序列化) Jackson2JsonRedisSerializer&lt;Object&gt; jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer&lt;&gt;(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); redisTemplate.setKeySerializer(jackson2JsonRedisSerializer); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashKeySerializer(jackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); return redisTemplate; }} 4. 开发 lua 脚本 很多应用的服务任务包含多步redis操作以及使用多个redis命令，这时你可以使用Redis结合Lua脚本，会为你的应用带来更好的性能。 另外包含在一个Lua脚本里面的redis命令具备原子性，当你面对高并发场景下的redis数据库操作时，可以有效避免多线程操作产生脏数据。 简单的示例 123456789101112local key = KEYS[1]local count = tonumber(ARGV[1])local time = tonumber(ARGV[2])local current = redis.call('get', key)if current and tonumber(current) &gt; count then return tonumber(current)endcurrent = redis.call('incr', key)if tonumber(current) == 1 then redis.call('expire', key, time)endreturn tonumber(current) 这个脚本其实不难，大概瞅一眼就知道干啥用的。KEYS 和 ARGV 都是一会调用时候传进来的参数，tonumber 就是把字符串转为数字，redis.call 就是执行具体的 redis 指令，具体流程是这样： 首先获取到传进来的 key 以及 限流的 count 和时间 time。通过 get 获取到这个 key 对应的值，这个值就是当前时间窗内这个接口可以访问多少次。如果是第一次访问，此时拿到的结果为 nil，否则拿到的结果应该是一个数字，所以接下来就判断，如果拿到的结果是一个数字，并且这个数字还大于 count，那就说明已经超过流量限制了，那么直接返回查询的结果即可。如果拿到的结果为 nil，说明是第一次访问，此时就给当前 key 自增 1，然后设置一个过期时间。最后把自增 1 后的值返回就可以了。 在一个 bean 中 加载这个 lua 脚本 1234567@Beanpublic DefaultRedisScript&lt;Long&gt; limitScript() { DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(&quot;lua/limit.lua&quot;))); redisScript.setResultType(Long.class); return redisScript;} 5. 注解解释 自定义切面 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Aspect@Componentpublic class RateLimiterAspect { private static final Logger log = LoggerFactory.getLogger(RateLimiterAspect.class); @Autowired private RedisTemplate&lt;Object, Object&gt; redisTemplate; @Autowired private RedisScript&lt;Long&gt; limitScript; @Before(&quot;@annotation(rateLimiter)&quot;) public void doBefore(JoinPoint point, RateLimiter rateLimiter) throws Throwable { String key = rateLimiter.key(); int time = rateLimiter.time(); int count = rateLimiter.count(); String combineKey = getCombineKey(rateLimiter, point); List&lt;Object&gt; keys = Collections.singletonList(combineKey); try { Long number = redisTemplate.execute(limitScript, keys, count, time); if (number==null || number.intValue() &gt; count) { throw new ServiceException(&quot;访问过于频繁，请稍候再试&quot;); } log.info(&quot;限制请求'{}',当前请求'{}',缓存key'{}'&quot;, count, number.intValue(), key); } catch (ServiceException e) { throw e; } catch (Exception e) { throw new RuntimeException(&quot;服务器限流异常，请稍候再试&quot;); } } public String getCombineKey(RateLimiter rateLimiter, JoinPoint point) { StringBuffer stringBuffer = new StringBuffer(rateLimiter.key()); if (rateLimiter.limitType() == LimitType.IP) { stringBuffer.append(IpUtils.getIpAddr(((ServletRequestAttributes) RequestContextHolder.currentRequestAttributes()).getRequest())).append(&quot;-&quot;); } MethodSignature signature = (MethodSignature) point.getSignature(); Method method = signature.getMethod(); Class&lt;?&gt; targetClass = method.getDeclaringClass(); stringBuffer.append(targetClass.getName()).append(&quot;-&quot;).append(method.getName()); return stringBuffer.toString(); }} 这个切面就是拦截所有加了 @RateLimiter 注解的方法，在前置通知中对注解进行处理。 首先获取到注解中的 key、time 以及 count 三个参数。 获取一个组合的 key，所谓的组合的 key，就是在注解的 key 属性基础上，再加上方法的完整路径，如果是 IP 模式的话，就再加上 IP 地址。以 IP 模式为例，最终生成的 key 类似这样：rate_limit:127.0.0.1-org.example.ratelimiter.controller.HelloController-hello（如果不是 IP 模式，那么生成的 key 中就不包含 IP 地址）。 将生成的 key 放到集合中。 通过 redisTemplate.execute 方法取执行一个 Lua 脚本，第一个参数是脚本所封装的对象，第二个参数是 key，对应了脚本中的 KEYS，后面是可变长度的参数，对应了脚本中的 ARGV。 将 Lua 脚本执行的结果与 count 进行比较，如果大于 count，就说明过载了，抛异常就行了。 好了，大功告成了。 这就是 主要的流程 demo 地址：https://gitee.com/sahura/redis_limit.git","link":"/2022/04/23/%E9%9C%80%E6%B1%82%E6%A1%88%E4%BE%8B/redis%20%E5%81%9A%E6%8E%A5%E5%8F%A3%E9%99%90%E6%B5%81/"},{"title":"单例模式","text":"volatilevolatile 是 Java 虚拟机提供的轻量级别的同步机制 保证可见性 不保证原子性 禁止指令重排（保证有序性） 原理volatile 可以保证线程可见性且提供了一定的有序性，但是无法保证原子性。在 JVM 底层是基于内存屏障实现的。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到 CPU 缓存中。如果计算机有多个CPU，每个线程可能在不同的 CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。 声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步，所以就不会有可见性问题。 对 volatile 变量进行写操作时，会在写操作后加一条 store 屏障指令，将工作内存中的共享变量刷新回主内存； 对 volatile 变量进行读操作时，会在写操作后加一条 load 屏障指令，从主内存中读取共享变量； 有 volatile 修饰的共享变量进行写操作时会多出第二行汇编代码，该句代码的意思是对原值加零，其中相加指令addl前有 lock 修饰。通过查IA-32架构软件开发者手册可知，lock前缀的指令在多核处理器下会引发两件事情： 将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会引起在其他CPU里缓存了该内存地址的数据无效 使用场景volatile 实现禁止指令重排优化，从而避免了多线程环境下程序出现乱序执行的现象。 还有一个我们最常见的多线程环境中 DCL(double-checked locking) 版本的单例模式中，就是使用了 volatile 禁止指令重排的特性。 1234567891011121314151617public class Singleton { private static volatile Singleton instance; private Singleton(){} // DCL public static Singleton getInstance(){ if(instance ==null){ //第一次检查 synchronized (Singleton.class){ if(instance == null){ //第二次检查 instance = new Singleton(); } } } return instance; }} 因为有指令重排序的存在，双端检索机制也不一定是线程安全的。因为instance = new Singleton(); 初始化对象的过程其实并不是一个原子的操作，它会分为三部分执行， 给 instance 分配内存 调用 instance 的构造函数来初始化对象 将 instance 对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 步骤 2 和 3 不存在数据依赖关系，如果虚拟机存在指令重排序优化，则步骤 2和 3 的顺序是无法确定的。如果A线程率先进入同步代码块并先执行了 3 而没有执行 2，此时因为 instance 已经非 null。这时候线程 B 在第一次检查的时候，会发现 instance 已经是 非null 了，就将其返回使用，但是此时 instance 实际上还未初始化，自然就会出错。所以我们要限制实例对象的指令重排，用 volatile 修饰（JDK 5 之前使用了 volatile 的双检锁是有问题的）。 您只能在有限的一些情形下使用 volatile 变量替代锁。要使 volatile 变量提供理想的线程安全，必须同时满足下面两个条件： 对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 其实就是在需要保证原子性的场景，不要使用 volatile。 volatile性能volatile 的读性能消耗与普通变量几乎相同，但是写操作稍慢，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。 引用《正确使用 volaitle 变量》一文中的话： 很难做出准确、全面的评价，例如 “X 总是比 Y 快”，尤其是对 JVM 内在的操作而言。（例如，某些情况下 JVM 也许能够完全删除锁机制，这使得我们难以抽象地比较 volatile 和 synchronized 的开销。）就是说，在目前大多数的处理器架构上，volatile 读操作开销非常低 —— 几乎和非 volatile 读操作一样。而 volatile 写操作的开销要比非 volatile 写操作多很多，因为要保证可见性需要实现内存界定（Memory Fence），即便如此，volatile 的总开销仍然要比锁获取低。 volatile 操作不会像锁一样造成阻塞，因此，在能够安全使用 volatile 的情况下，volatile 可以提供一些优于锁的可伸缩特性。如果读操作的次数要远远超过写操作，与锁相比，volatile 变量通常能够减少同步的性能开销。","link":"/2022/05/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%85%B3%E9%94%AE%E5%AD%97/"},{"title":"单例模式","text":"一、介绍单例模式就是只有一个实例，她负责创建自己的对象，这个类提供一种访问起唯一的对象方式，不需要实例化类的对象。 核心内容： 私有化构造器。外部不能通过 new 创建对象 对象的创建交给自己完成 提供一个公共静态方法，返回唯一的实例。 最关键就是创建对象的过程不同，有不同的模式。 创建对象的方式 new 创建 反射机制创建 clone 创建 序列化创建 针对这些方式 就有可能打破传统的单例方式 二、饿汉式2.1 静态常量创建123456789101112131415public class HungryStaticConstant { // 1. 私有化构造器。外部不能通过 new 创建对象 private HungryStaticConstant() { } // 2. 对象的创建交给自己完成, 放在静态常量中 private static final HungryStaticConstant singleton = new HungryStaticConstant(); // 3. 提供一个公共静态方法，返回唯一的实例。 public static HungryStaticConstant getSingleton() { return singleton; }} 对象实例在初始化的时候就已经建好了。不管有没有用到，都建好了再说。不存在线程安全问题，坏处就是浪费内存空间。 验证单例有效性 123456@Testvoid contextLoads() { HungryStaticConstant singleton = HungryStaticConstant.getSingleton(); HungryStaticConstant singleton1 = HungryStaticConstant.getSingleton(); System.out.println(singleton==singleton1);} 结果当然是 true。正常使用时没有问题。但是如果通过 2.1.1 反射对象打破单例模式123456789101112131415161718192021@Testvoid contextLoads1() { HungryStaticConstant singleton = HungryStaticConstant.getSingleton(); Class hungryStaticConstantClass = HungryStaticConstant.class; try { Constructor&lt;HungryStaticConstant&gt; declaredConstructor = hungryStaticConstantClass.getDeclaredConstructor(); // 通过反射拿到单例的私有构造器，创建了新的对象 declaredConstructor.setAccessible(true); HungryStaticConstant hungryStaticConstant = declaredConstructor.newInstance(); System.out.println(singleton == hungryStaticConstant); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (InvocationTargetException e) { e.printStackTrace(); } catch (NoSuchMethodException e) { e.printStackTrace(); }} 结果是 false 为了防止单例模式被反射入侵。可以在构造方法中加锁 12345678// 1. 私有化构造器。外部不能通过 new 创建对象private HungryStaticConstant() { synchronized (HungryStaticConstant.class){ if (singleton == null){ throw new RuntimeException(&quot;单例模式被入侵&quot;); } }} 2.1.2 序列化方式打破单例模式HungryStaticConstant implements Serializable实现这个接口。那么通过序列化的操作也是可以打破单例模式的。 1234567891011121314151617181920@Testvoid contextLoads2() { HungryStaticConstant singleton = HungryStaticConstant.getSingleton(); try { ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); // 序列化对象 objectOutputStream.writeObject(singleton); ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); // 反序列化创建对象 HungryStaticConstant staticConstant = (HungryStaticConstant) objectInputStream.readObject(); System.out.println(staticConstant == singleton); //false } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); }} 序列化对象 –》 反序列化对象 打破单例模式 添加readResolve() ：反序列化readObject()方法获取对象时会去寻找readResolve()方法，如果该方法不存在则直接返回新对象，如果该方法存在则按该方法的内容返回对象，以确保如果我们之前实例化了单例对象，就返回该对象。如果我们之前没有实例化单例对象，则会返回null。 123456public class HungryStaticConstant implements Serializable{ private Object readResolve(){ return singleton; }} 2.1.3 clone 对象clone时，需要已经有一个分配了内存的源对象，创建新对象时，首先应该分配一个和源对象一样大的内存空间。 要调用clone方法需要实现Cloneable接口，由于clone方法是protected的。 提供一个方法，此时返回的对象也不是不同的 1234567public class HungryStaticConstant implements Cloneable{ // 4. 克隆对象 @SneakyThrows public static HungryStaticConstant getCloneSingleton() { return (HungryStaticConstant) singleton.clone(); }} 三、懒汉式3.1 双重检查锁DCL12345678910111213141516171819202122public class LazySingleton { private static volatile LazySingleton singleton; private LazySingleton() { } // 增加锁保证线程安全，避免在多线程的情况下创建多个实例，打破单利模式。 public synchronized static LazySingleton getSingleTon(){ // 第一次检验 if (singleton == null){ synchronized (LazySingleton.class){ // 第二次检验 if (singleton == null){ singleton = new LazySingleton(); } } } return singleton; }} 先来第一次判断，看是否已经创建了单例对象，如果创建直接返回即可，不存在线程问题，也就避免了性能问题不用等待。 如果没有创建，那么我们进入加锁的代码块。这里需要注意，因为多线程的情况下，都可能进入第二次校验。 此时还没有创建单例对象，而进入的线程排队准备创建案例对象。所以我们此时进行第二次校验，那么只要有一个线程常见成功。由于这里是加锁的，后面的线程顺序执行的时候就不会再创建单例。既可以保证单例模式的安全。 因为这种模式我们只要需要等待一次单例的获取，所以基本上不存在获取单例的性能问题。 3.2 静态内部类123456789101112131415161718public class InnerLazySingleton { private InnerLazySingleton(){ } private static class Instance{ public Instance(){ } private static final InnerLazySingleton singleton = new InnerLazySingleton(); } // 提供静态的方法返回单例。 public static InnerLazySingleton getInstance(){ System.out.println(&quot;调用getInstance发方法&quot;); return Instance.singleton; }} 四、枚举123456789101112131415161718192021public class EnumSingleton { private EnumSingleton(){} private enum Singleton{ INSTANCE; private final EnumSingleton singleton; // JVM 会保证只会调用一次初始化方法 Singleton(){ singleton = new EnumSingleton(); } private EnumSingleton getSingleton(){ return singleton; } } public static EnumSingleton getInstance(){ return Singleton.INSTANCE.getSingleton(); }} 使用枚举除了线程安全和防止反射调用构造器之外，还提供了自动序列化机制，防止反序列化的时候创建新的对象。 演示项目地址：https://gitee.com/sahura/designPattern.git","link":"/2022/05/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"title":"mybatis用法","text":"一、一对一 查询举例：每本书 都有一个作者，作者有自己的属性。 想要得要这样一个数据结构: 1234567891011{ &quot;id&quot;: 1, &quot;name&quot;: &quot;A三国演义&quot;, &quot;author&quot;: { &quot;id&quot;: 2, &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 25 }} 1.1 数据库 定义 2张表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*Navicat MySQL Data TransferSource Server : MySQLSource Server Version : 50723Source Host : localhost:3306Source Database : javaTarget Server Type : MYSQLTarget Server Version : 50723File Encoding : 65001Date: 2022-05-09 10:49:32*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for author-- ----------------------------DROP TABLE IF EXISTS `author`;CREATE TABLE `author` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4;-- ------------------------------ Records of author-- ----------------------------INSERT INTO `author` VALUES ('1', '张三', '18');INSERT INTO `author` VALUES ('2', '李四', '25');-- ------------------------------ Table structure for book-- ----------------------------DROP TABLE IF EXISTS `book`;CREATE TABLE `book` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `aid` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4;-- ------------------------------ Records of book-- ----------------------------INSERT INTO `book` VALUES ('1', 'A三国演义', '2');INSERT INTO `book` VALUES ('2', 'B西游记', '1'); 1.2 查询1.2.1 entitybook 123456@Datapublic class Book { private Integer id; private String name; private Author author;} author 12345678@Datapublic class Author { private Integer id; private String name; private Integer age;} 1.2.2 mapping1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.example.mybatis.mapper.BookMapper&quot;&gt; &lt;resultMap id=&quot;BookWithAuthor&quot; type=&quot;com.example.mybatis.entity.Book&quot;&gt; &lt;result column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;name&quot; /&gt; &lt;association property=&quot;author&quot; javaType=&quot;com.example.mybatis.entity.Author&quot;&gt; &lt;result column=&quot;aid&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;aname&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;aage&quot; jdbcType=&quot;INTEGER&quot; property=&quot;age&quot; /&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;getBookById&quot; resultMap=&quot;BookWithAuthor&quot;&gt; SELECT b.id,b.`name`,a.age as aage,a.`name` as aname,a.id as aid from book b,author a where b.aid = a.id and b.id = #{id} &lt;/select&gt;&lt;/mapper&gt; 查询 SQL 中，首先应该做好 一对一 查询，然后把返回值定义成 resultMap，通过 map 来定义结果映射关系 其中 association节点来描述 一对一的关系 实际项目中，每次返回的数据类型可能都会有差异，这就需要定义多个 resultMap，而这多个 resultMap 中，又有一部份属性是相同的，所以，我们可以将相同的部分抽出来，做成一个公共的模板，然后被其他 resultMap 继承，优化之后的 mapper 如下： 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;org.javaboy.mybatis.mapper.BookMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;org.javaboy.mybatis.model.Book&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;/resultMap&gt; &lt;resultMap id=&quot;BookWithAuthor&quot; type=&quot;org.javaboy.mybatis.model.Book&quot; extends=&quot;BaseResultMap&quot;&gt; &lt;association property=&quot;author&quot; javaType=&quot;org.javaboy.mybatis.model.Author&quot;&gt; &lt;id column=&quot;aid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;aname&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;aage&quot; property=&quot;age&quot;/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;getBookById&quot; resultMap=&quot;BookWithAuthor&quot;&gt; SELECT b.*,a.`age` AS aage,a.`id` AS aid,a.`name` AS aname FROM book b,author a WHERE b.`aid`=a.`id` AND b.`id`=#{id} &lt;/select&gt;&lt;/mapper&gt; 1.2.3 mapper12345@Repositorypublic interface BookMapper { Book getBookById(Integer id);} 1.3 懒加载需要查询关联信息时，使用 Mybatis 懒加载特性可有效的减少数据库压力，首次查询只查询主表信息，关联表的信息在用户获取时再加载。 开启懒加载 配置文件增加 1234# mybatis 懒加载 true 开启懒加载mybatis.configuration.lazy-loading-enabled=true# 每种属性按需加载，不调用就不加载。true：只要调用任意具有懒加载特性的对象的任意一个属性将完整加载整个对象，即触发级联效果。false：只加载调用的属性，不调用的属性不加载。mybatis.configuration.aggressive-lazy-loading=false 1.3.1 mapper12Book getBookById2(Integer id);Author getAuthorById(Integer id); 1.3.2 mapping123456789101112131415&lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.example.mybatis.entity.Book&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt;&lt;/resultMap&gt;&lt;resultMap id=&quot;BookWithAuthor2&quot; type=&quot;com.example.mybatis.entity.Book&quot; extends=&quot;BaseResultMap&quot;&gt; &lt;association property=&quot;author&quot; javaType=&quot;com.example.mybatis.entity.Author&quot; select=&quot;com.example.mybatis.mapper.BookMapper.getAuthorById&quot; column=&quot;aid&quot; fetchType=&quot;lazy&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;getBookById2&quot; resultMap=&quot;BookWithAuthor2&quot;&gt; select * from book where id=#{id};&lt;/select&gt;&lt;select id=&quot;getAuthorById&quot; resultType=&quot;com.example.mybatis.entity.Author&quot;&gt; select * from author where id=#{aid};&lt;/select&gt; 1.3.3 注意事项@JsonIgnoreProperties(value = { “handler” }) 使用懒加载进行级联查询 会默认生成 handler 处理对象此时会报 json 格式化错误 二、一对多查询举例：用户和角色的关系，一个用户可以有多个角色 association 描述一对一 collection 描述一对多 2.1 数据库表结构数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/*Navicat MySQL Data TransferSource Server : MySQLSource Server Version : 50723Source Host : localhost:3306Source Database : javaTarget Server Type : MYSQLTarget Server Version : 50723File Encoding : 65001Date: 2022-05-09 15:02:49*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for author-- ----------------------------DROP TABLE IF EXISTS `author`;CREATE TABLE `author` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4;-- ------------------------------ Records of author-- ----------------------------INSERT INTO `author` VALUES ('1', '张三', '18');INSERT INTO `author` VALUES ('2', '李四', '25');-- ------------------------------ Table structure for book-- ----------------------------DROP TABLE IF EXISTS `book`;CREATE TABLE `book` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `aid` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4;-- ------------------------------ Records of book-- ----------------------------INSERT INTO `book` VALUES ('1', 'A三国演义', '2');INSERT INTO `book` VALUES ('2', 'B西游记', '1');INSERT INTO `book` VALUES ('3', 'Cxxx', '4');-- ------------------------------ Table structure for role-- ----------------------------DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(32) DEFAULT NULL, `nameZh` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;-- ------------------------------ Records of role-- ----------------------------INSERT INTO `role` VALUES ('1', 'dba', '数据库管理员');INSERT INTO `role` VALUES ('2', 'admin', '系统管理员');INSERT INTO `role` VALUES ('3', 'user', '用户');-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(32) NOT NULL AUTO_INCREMENT, `userName` varchar(32) NOT NULL, `passWord` varchar(50) NOT NULL, `realName` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('1', 'admin', 'admin', '管理员');INSERT INTO `user` VALUES ('2', 'abc', '123', '用户');-- ------------------------------ Table structure for user_role-- ----------------------------DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `id` int(11) NOT NULL AUTO_INCREMENT, `uid` int(11) DEFAULT NULL, `rid` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user_role-- ----------------------------INSERT INTO `user_role` VALUES ('1', '1', '1');INSERT INTO `user_role` VALUES ('2', '1', '2');INSERT INTO `user_role` VALUES ('3', '2', '2');INSERT INTO `user_role` VALUES ('4', '3', '3'); 2.2 查询2.2.1 entity123456789101112131415161718import java.util.List;@Datapublic class User { private Integer id; private String userName; private String passWord; private String realName; private List&lt;Role&gt; roles;}@Datapublic class Role { private Integer id; private String name; private String nameZh;} 2.2.2 mapper12345678@Repositorypublic interface UserMapper { User getUser(int id); User getUserWithRoleById(int id);} 2.2.3 mappingcollection 描述了 对象 一对多的关系 1234567891011121314&lt;resultMap id=&quot;UserWithRole&quot; type=&quot;com.example.mybatis.entity.User&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;userName&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;userName&quot; /&gt; &lt;result column=&quot;passWord&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;passWord&quot; /&gt; &lt;collection property=&quot;roles&quot; ofType=&quot;com.example.mybatis.entity.Role&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;rid&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;rname&quot;/&gt; &lt;result property=&quot;nameZh&quot; column=&quot;rnameZH&quot;/&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;select id=&quot;getUserWithRoleById&quot; resultMap=&quot;UserWithRole&quot;&gt; SELECT u.*,r.`id` AS rid,r.`name` AS rname,r.`nameZh` AS rnameZh FROM USER u,role r,user_role ur WHERE u.`id`=ur.`uid` AND ur.`rid`=r.`id` AND u.`id`=#{id}&lt;/select&gt; 2.2.4 结果 三、条件查询（where and 动态 SQL）示例：多个条件组合查询（id, userName） 查询用户角色信息 基础代码承上 3.1 核心 mapping1234567891011&lt;select id=&quot;getUserIf&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select * from user &lt;where&gt; &lt;if test=&quot;id != null&quot;&gt; id = #{id} &lt;/if&gt; &lt;if test=&quot;userName != null&quot;&gt; username = #{userName} &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 3.2 mapper123456789@Repositorypublic interface UserMapper { User getUser(int id); User getUserWithRoleById(int id); // 条件查询 List&lt;User&gt; getUserIf(User user);} 四、ForEach 查询（where in 动态 SQL）示例：多个相同条件查询（id1,id2） 查询用户角色信息 核心 代码 4.1 mapping123456789&lt;select id=&quot;getUserIn&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select * from user &lt;where&gt; &lt;foreach collection=&quot;ids&quot; item = &quot;id&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;or&quot;&gt; id = #{id} &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 4.2 mappper1List&lt;User&gt; getUserIn(List&lt;Integer&gt; ids); 4.3 service123456789public List&lt;User&gt; getUserIn(String ids){ final String[] split = ids.split(&quot;,&quot;); List&lt;Integer&gt; _ids = new ArrayList&lt;&gt;(16); for (String s : split) { _ids.add(Integer.valueOf(s)); } return userMapper.getUserIn(_ids);}","link":"/2022/04/18/ORM/mybatis/mybatis%20%E7%94%A8%E6%B3%95/"},{"title":"docker 安装 nginx","text":"1 docker 安装 nginx1.1.1 拉取最新镜像1docker pull nginx:latest 查看是否安装 nginx 1docker images 1.1.2 启动 nginx1docker run --name nginx -p 8081:80 -d nginx 以本地端口 8081 映射 容器80 端口 启动 nginx 1.1.2 创建挂载目录123456mkdir -p /usr/local/nginx/conf/conf.dmkdir -p /usr/local/nginx/htmlmkdir -p /usr/local/nginx/log 1.1.3 文件夹授权123456chmod 777 /usr/local/nginx/conf/conf.dchmod 777 /usr/local/nginx/htmlchmod 777 /usr/local/nginx/log 进入容器 123docker exec -it nginx bash# 找配置文件目录find / -name nginx.conf 找到配置文件目录 12/etc/nginx/nginx.conf 返回宿主机,吧配置文件复制到宿主机上 方便管理 12345docker cp nginx:/etc/nginx/nginx.conf /usr/local/nginx/confdocker cp nginx:/etc/nginx/conf.d /usr/local/nginx/conf/conf.ddocker cp nginx:/usr/share/nginx/html /usr/local/nginx/docker cp nginx:/var/log/nginx /usr/local/nginx/log 修改 本地 default.conf 端口 为81 ，验证本地修改是否生效 启动一个 nginx1 容器 端口映射 8088端口 到 容器 81 端口 1234567docker run -it -p 8088:81--name nginx1-v /usr/local/nginx/conf/nginx.conf:/etc/nginx/nginx.conf-v /usr/local/nginx/conf/conf.d/default.conf:/etc/nginx/conf.d/default.conf-v /usr/local/nginx/log:/var/log/nginx-v /usr/local/nginx/html:/usr/share/nginx/html-d nginx 1.1.4 挂载配置文件启动1docker run -it -p 8088:81 --name nginx1 -v /usr/local/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /usr/local/nginx/conf/conf.d/default.conf:/etc/nginx/conf.d/default.conf -v /usr/local/nginx/log:/var/log/nginx -v /usr/local/nginx/html:/usr/share/nginx/html -d nginx docker reload 配置文件 1docker exec nginx nginx -s reload 1.1.5 成功","link":"/2022/05/17/%E4%B8%AD%E9%97%B4%E4%BB%B6/nginx/docker%20%E5%AE%89%E8%A3%85%20nginx/"},{"title":"springboot整合mybatis","text":"一、整合 mybatis1.1 创建 springboot 项目 1.2 添加需要依赖 1.3 修改配置文件resource 文件下 application.properties 123456789101112131415# 服务端口号server.port=8088# 数据库配置spring.datasource.username=rootspring.datasource.password=123spring.datasource.url=jdbc:mysql://localhost:3306/java?useUnicode=true&amp;characterEncoding=UTF-8# mybatis 配置 扫描Mapper.xmlmybatis.mapper-locations=classpath:mapping/*Mapper.xml# mybatis 实体类包位置mybatis.type-aliases-package=com.example.mybatis.entity# showSQL 开启 sql 打印logging.level.com.example.mybatis.mapper=debug 1.4 创建目录实现业务流程 准备工作：准备一个数据库，建表语句 1234567891011121314151617181920212223242526272829303132333435/*Navicat MySQL Data TransferSource Server : MySQLSource Server Version : 50723Source Host : localhost:3306Source Database : javaTarget Server Type : MYSQLTarget Server Version : 50723File Encoding : 65001Date: 2022-05-07 10:05:49*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(32) NOT NULL AUTO_INCREMENT, `userName` varchar(32) NOT NULL, `passWord` varchar(50) NOT NULL, `realName` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES ('3', 'admin', 'admin', '管理员');INSERT INTO `user` VALUES ('4', 'abc', '123', '用户'); entity 目录1234567891011import lombok.Data;@Datapublic class User { private Integer id; private String userName; private String passWord; private String realName;} mapper 目录123456@Repositorypublic interface UserMapper { User getUser(int id);} mapping 目录12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.example.mybatis.mapper.UserMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.example.mybatis.entity.User&quot;&gt; &lt;result column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;userName&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;userName&quot; /&gt; &lt;result column=&quot;passWord&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;passWord&quot; /&gt; &lt;result column=&quot;realName&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;realName&quot; /&gt; &lt;/resultMap&gt; &lt;select id=&quot;getUser&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select * from user where id = #{id} &lt;/select&gt;&lt;/mapper&gt; service 目录123456789@Servicepublic class UserService { @Autowired UserMapper userMapper; public User getUser(int id){ return userMapper.getUser(id); }} controller 目录123456789101112@RestController@RequestMapping(&quot;/user&quot;)public class UserController { @Autowired UserService userService; @GetMapping(&quot;getUser/{id}&quot;) public User getUser(@PathVariable int id){ return userService.getUser(id); }} 项目启动入口 添加 Mapperscan12345678910@SpringBootApplication@MapperScan(&quot;com.example.mybatis.mapper&quot;)public class MybatisApplication { public static void main(String[] args) { SpringApplication.run(MybatisApplication.class, args); }} 启动项目 访问接口http://127.0.0.1:8088/user/getUser/3 1.5 项目地址https://gitee.com/sahura/mybatis.git","link":"/2022/04/19/ORM/mybatis/springboot%E6%95%B4%E5%90%88mybatis/"},{"title":"mybatis分页","text":"1. 原生 SQL 关键字实现分页查询分页数据的基本属性 分页起始页 每页记录数 总条数 返回记录集合 1.1 自定义 Pager 对象描述分页数据1234567891011import lombok.Data;import java.util.List;@Datapublic class Pager &lt;T&gt;{ private int page;//分页起始数 private int size;//每页记录数 private long total;//总条数 private List&lt;T&gt; rows;//集合数} 1.2 limit 关键字实现逻辑分页Dao层 userDao 增加 2个方法 123List&lt;User&gt; findByPager(Map&lt;String, Object&gt; params);long count(); mybatis Mapper.xml 1234567&lt;select id=&quot;findByPager&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select * from user LIMIT #{page},#{size}&lt;/select&gt;&lt;select id=&quot;count&quot; resultType=&quot;long&quot;&gt; select COUNT(1) from user&lt;/select&gt; service 实现逻辑分页 123456789101112public Pager&lt;User&gt; findByPager(int page, int size){ Map&lt;String, Object&gt; params = new HashMap&lt;&gt;(8); params.put(&quot;page&quot;, (page-1)*size); params.put(&quot;size&quot;, size); Pager&lt;User&gt; pager = new Pager&lt;User&gt;(); List&lt;User&gt; list = userMapper.findByPager(params); pager.setRows(list); pager.setTotal(userMapper.count()); pager.setPage(page); pager.setSize(size); return pager;} 1.3 limit 性能问题分页查询时，我们会在 LIMIT 后面传两个参数，一个是偏移量（offset），一个是获取的条数（limit）。当偏移量很小时，查询速度很快，但是当 offset 很大时，查询速度就会变慢。 优化的手段就是 索引覆盖 1.3.1 SQL 语句优化1123&lt;select id=&quot;findByPager&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select * from user where id in (select id from user LIMIT #{page},#{size})&lt;/select&gt; This version of MySQL doesn’t yet support ‘LIMIT &amp; IN/ALL/ANY/SOME subquery’ 1.3.2 SQL 语句优化2123&lt;select id=&quot;findByPager&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; select t1.* from user t1 where t1.id in (select t2.id from (select id from user LIMIT #{page},#{size}) as t2)&lt;/select&gt; 1.3.3 SQL 语句优化31234&lt;select id=&quot;findByPager&quot; resultType=&quot;com.example.mybatis.entity.User&quot;&gt; SELECT * FROM user t1 JOIN (SELECT id from user limit #{page},#{size}) t2 ON t1.id = t2.id &lt;/select&gt; 这条SQL的含义是，通过自连接与join定位到目标 ids，然后再将数据取出。在定位目标 ids时，由于 SELECT的元素只有主键 ID，因此MySQL只需在索引中，就能定位到目标 ids，不用在数据文件上进行查找。因而，查询效率非常高。 2. 数组分页原理：进行数据库查询操作时，获取到数据库中所有满足条件的记录，保存在应用的临时数组中，再通过List的subList方法，获取到满足条件的所有记录。 3. RowBounds(int offset,int limit) 分页RowBounds 表面是在“所有”数据中检索数据，其实并非是一次性查询出所有数据，因为 MyBatis 是对 jdbc 的封装，在 jdbc 驱动中有一个 Fetch Size 的配置，它规定了每次最多从数据库查询多少条数据，假如你要查询更多数据，它会在你执行 next()的时候，去查询更多的数据。就好比你去自动取款机取 10000 元，但取款机每次最多能取 2500 元，所以你要取 4 次才能把钱取完。只是对于 jdbc 来说，当你调用 next()的时候会自动帮你完成查询工作。这样做的好处可以有效的防止内存溢出。 mappper.xml 1234&lt;select id=&quot;getUserByRowBounds&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from user&lt;/select&gt; dao 1List&lt;User&gt; getUserByRowBounds(RowBounds rowBounds); service 12345public List&lt;User&gt; getUserByRowBounds(int page, int size){ RowBounds rowBounds = new RowBounds((page-1)*size,size); return userMapper.getUserByRowBounds(rowBounds);} 4. Pagehelper 插件分页pom.xml 备注：低版本 pagehelper 启动可能会有 循环依赖问题 1234567&lt;!--pagehelper --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.4.2&lt;/version&gt;&lt;/dependency&gt; application.properties 12345#pagehelper分页插件配置pagehelper.helperDialect=mysqlpagehelper.reasonable=truepagehelper.supportMethodsArguments=truepagehelper.params=count=countSql mapper.xml 12345678910111213&lt;sql id=&quot;base_column_list&quot; &gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; id, username,password&lt;/sql&gt;&lt;select id=&quot;getUserByPagehelper&quot; resultMap=&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;base_column_list&quot;&gt;&lt;/include&gt; from user&lt;/select&gt; dao 1Page&lt;User&gt; getUserByPagehelper(); service 1234567import com.github.pagehelper.Page;import com.github.pagehelper.PageHelper; public Page&lt;User&gt; getUserByPagehelper(int page, int size){ PageHelper.startPage(page, size); return userMapper.getUserByPagehelper(); } 4.1 PageHelper 返回数据问题正常应该取 id 9-10 的数据 ，结果却返回 最后一条分页数据。（数据库中 7条数据，返回了 第7条数据 offset 6 limit 2） 问题原因 ： 12 # 结果合理化pagehelper.reasonable=false 这个配置应该关闭 调整之后 数据返回正常 超出分页 范围返回为空","link":"/2022/04/17/ORM/mybatis/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"},{"title":"nginx","text":"1. nginx 介绍nginx 是一款 轻量级别 Web 服务器，反向代理服务，占用内存少，启动极快，高并发能力强，互联网项目广泛使用 目前主流技术架构图 2. 正向代理正向代理代理的是客户端 由于 防火墙的原因，无法直接访问服务器，可以借助 VPN 实现。 3. 反向代理反向代理代理的是服务端 用户访问百度的时候，其实会经过代理服务器的转发在真实服务器 4. nginx 工作模式 Master-Worker 模式 启动 Nginx 后，在 启动端口启动 Socket 服务监听；涉及 Master进程和Worker进程 Master进程的作用是？ 读取并验证配置文件nginx.conf；管理worker进程； Worker进程的作用是？ 每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。思考：Nginx如何做到热部署？ 所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop） 通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一： 修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点valatile的味道） 方案二： 修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ 5. nginx 挂了怎么办？nginx 作为网关入口，如果出现单点问题，显然不可接受的。 Keepalived + Nginx 实现高可用 Keepalived 是 高可用解决方案，主要是用来防止服务器单点故障, 通过配合 Nginx 配合实现 Web 服务的高可用 Keepalived + Nginx 实现高可用思路 请求不要直接打到 Nginx 上，先通过 KeepAlived 虚拟 IP,VIP KeepAlived 应该能够监控 Nginx 的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换） 6. nginx 安装6.1 nginx 安装6.1.1 从官网下载 安装包1wget http://nginx.org/download/nginx-1.21.6.tar.gz 6.1.2 编译安装解压 1tar -zxvf nginx-1.21.6.tar.gz 编译安装 12345cd nginx-1.21.6./configure# 编译安装make &amp;&amp; make install 6.1.3 可能存在的问题,需要添加外部依赖123456./configure: error: the HTTP rewrite module requires the PCRE library.yum -y install pcre-devel./configure: error: SSL modules require the OpenSSL library.yum -y install openssl openssl-devel 找到nginx的目录 123whereis nginx#nginx: /usr/local/nginx 6.1.4 nginx命令123456nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启nginx -s reopen # 重启 Nginxnginx -s stop # 快速关闭nginx -s quit # 等待工作进程处理完成后关闭nginx -T # 查看当前 Nginx 最终的配置nginx -t # 检查配置是否有问题 7. nginx.conf主要分为几大部分内容： main（全局设置）：设置的指令将影响其他所有设置；server（主机设置）：指令主要用于指定主机和端口、upstream（负载均衡服务器设置）：指令主要用于负载均衡，设置一系列的后端服务器location（URL匹配特定位置的设置）：用于匹配网页位置。 nginx 提高速度的特点：动静分离 ，静态资源直接给 nginx管理，请求转发给后端 nginx 做 IP 访问控制 以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 在nginx 安装目录下 可以创建 ip.conf 文件，后续增加 屏蔽 ip 只需要编辑这个文件 实现这样的应用，除了几个IP外，其他全部拒绝，这样写 1234# 准许访问allow 127.0.0.2# 屏蔽访问deny all; 在 nginx 配置中加入配置 1include ip.conf; 单独网站屏蔽IP的方法，把include ip.conf; 放到网址对应的在server{}语句块 所有网站屏蔽IP的方法，把include ip.conf; 放到http {}语句块。 7.1 nginx.conf 典型配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# main段配置信息user nginx; # 运行用户，默认即是nginx，可以不进行设置worker_processes auto; # Nginx 进程数，一般设置为和 CPU 核数一样error_log /var/log/nginx/error.log warn; # Nginx 的错误日志存放目录pid /var/run/nginx.pid; # Nginx 服务启动时的 pid 存放位置# events段配置信息events { use epoll; # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的) worker_connections 1024; # 每个进程允许最大并发数}# http段配置信息# 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置http { # 设置日志模式 log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log /var/log/nginx/access.log main; # Nginx访问日志存放位置 sendfile on; # 开启高效传输模式 tcp_nopush on; # 减少网络报文段的数量 tcp_nodelay on; keepalive_timeout 65; # 保持连接的时间，也叫超时时间，单位秒 types_hash_max_size 2048; include /etc/nginx/mime.types; # 文件扩展名与类型映射表 default_type application/octet-stream; # 默认文件类型 include /etc/nginx/conf.d/*.conf; # 加载子配置项 # server段配置信息 server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 # location段配置信息 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 deny 172.168.22.11; # 禁止访问的ip地址，可以为all allow 172.168.33.44；# 允许访问的ip地址，可以为all } error_page 500 502 503 504 /50x.html; # 默认50x对应的访问页面 error_page 400 404 error.html; # 同上 }}","link":"/2022/05/17/%E4%B8%AD%E9%97%B4%E4%BB%B6/nginx/nginx/"},{"title":"docker安装rocketmq","text":"一、 安装演示环境 linux 1.1 环境检查 jdk 版本 1.8 +1java -version 1.2 安装 jdk 1.81yum install -y java-1.8.0-openjdk.x86_64 1.3 java 环境准备就绪 1.4 下载 解压123wget https://github.com/apache/incubator-rocketmq/releasesunzip rocketmq-all-4.8.0-bin-release.zip 1.5 脚本文件配置调整rocketmq lianggel脚本文件默认是设置的8GB内存，如果是本地搭建的虚拟机或买的云服务器没有配置这么大内存，rocketmq 服务会启动失败，需要修改一下配置。如下有两个文件需要修改 修改 bin/runserver.sh 配置 1234#JAVA_OPT=&quot;${JAVA_OPT} -server -Xms4g -Xmx4g -Xmn2g-XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;#调整为（依情况而定）JAVA_OPT=&quot;${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; 修改 bin/runbroker.sh 1234#JAVA_OPT=&quot;${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g&quot;#调整为（依情况而定）JAVA_OPT=&quot;${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m&quot; 1.6 启动服务要在安装目录下 /bin 执行 Nameserver-服务端 执行命令 ： 1nohup sh mqnamesrv &amp; Broker-客户端（消费端）执行命令 ： 1nohup sh mqbroker -n localhost:9876 &amp; nohup的作用 nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时或者关闭客户端该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。 简单理解:nohup运行命令可以使命令永久的执行下去，和用户终端没有关系，例如我们断开SSH连接都不会影响他的运行，注意了nohup没有后台运行的意思； &amp;的作用&amp;是指在后台运行，但当用户退出(挂起)的时候，命令自动也跟着退出 注意nohup COMMAND &amp; 这样就能使命令永久的在后台执行nohup可以使用Ctrl+C结束掉，而&amp;使用Ctrl+C则结束不掉，nohup不受终端关闭，用户退出影响，而&amp;则受终端关闭，用户退出影响 1.7 查看启动日志Nameserver-服务端 1tail -f ~/logs/rocketmqlogs/namesrv.log Broker-客户端（消费端） 1tail -f ~/root/logs/rocketmqlogs/broker.log 1.8 jps 查看启动情况 jps 命令无法使用情况 1yum install java-1.8.0-openjdk-devel.x86_64 1.9 .关闭rocketMQ要在安装目录下 /bin 执行 Nameserver-服务端 1sh mqshutdown namesrv Broker-客户端（消费端） 1sh mqshutdown broker 二、控制台安装2.1 下载 rocketmq 控制台服务器编译 前置条件 安装 mvn 版本 3.2.* 以上 1wget https://github.com/apache/rocketmq-dashboard/archive/refs/heads/master.zip 解压 进入 1unzip rocketmq-dashboard-master 修改 src/main/resources中的application.yml 配置文件。 原来的端口号为 8080 ，修改为一个不常用的 指定RocketMQ的name server地址 123456789# 配置 application.yml namesrvAddres为自己的注册中心 rocketmq: config: namesrvAddrs: - 127.0.0.1:9876server: port: 8081 回到 rocketmq-dashboard-master 编译打包 123cd /rocketmq-dashboard-mastermvn clean package -Dmaven.test.skip=true 打好包后在target目录下会生成可执行的jar包 1java -jar target/rocketmq-dashboard-1.0.1-SNAPSHOT.jar 基本环境搭建成功","link":"/2022/05/12/%E4%B8%AD%E9%97%B4%E4%BB%B6/rocketmq/rocketmq/"},{"title":"负载均衡","text":"1. 介绍负载均衡，就是根据请求的信息不同，来决定怎么转发流量 四层负载均衡，就是根据请求的ip+端口，根据设定的规则，将请求转发到后端对应的IP+端口上。 七层负载均衡，则是在四层基础上，再去考虑应用层的特征。比如说一个WEB服务器的负载均衡，除了根据IP+80端口来判断之外，还可以根据七层URL，浏览器类别，来决定如何去转发流量。 四层交换机主要分析IP层和TCP/UDP层，实现四层流量负载，这种负载不关心七层的应用协议。七层的交换机除了支持四层之外，还要分析应用层，如HTTP协议、URL、cookie等信息。四层常见软件是haproxy，LVS，七层常见软件是nginx。 四层负载均衡主要是为了转发，七层负载均衡是内容交换 Nginx优缺点： 优点：开源软件，简单易部署；功能强大，七层负载基本可以满足所有需求。 缺点：仅支持http，https，Email协议；对后端服务器的检测，仅通过IP+端口来检查，不可以通过URL来检查；不支持会话session保持的一致行，但可以通过IP+HASH来解决。 LVS优缺点： 优点：工作在4层，仅做分发作用，没有流量产生，因此。负载性能最强，对内存和cpu消耗率更低； 缺点：不支持正则表达式，不支持动静分离。 haproxy优缺点 优点：支持session会话保持一致，四层和七层都支持；支持通过URL来检测后端服务器的状态。 缺点：在七层转发支持上，不如nginx强大。","link":"/2022/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"title":"String","text":"StringStringBuffer，StringBuilder区别是什么？标准答案：前者线程安全，后者线程不安全。 Java 语言设计之初，就是为了替代（当时）极其复杂原始的C/C++，其中的一个大改进就是，把并发（多线程）的理念贯彻到了语言的各个角落。 例如，当初 Java 设计者将 Java 语言设计成所有对象都支持作为 synchronized 代码块的互斥锁使用，还把 wait、notify 等 final 方法放到万物之源 Object 类。为什么最初几乎所有可变对象如 StringBuffer、Vector、Hashtable、ByteArray{Input,Output}Stream 等都要设计成线程安全的？ 因为当初的语言设计者认为多线程是万金油，这些可变对象几乎都会被多个线程同时修改，所以为了避免 Race Condition，也为了避免程序员频繁使用 synchronized 代码块造成可读性和易用性降低，就把它们统统设计成“线程安全”的，让保持同步的脏活累活都留给标准库干，反正也不影响单线程下的准确性，无非损失亿点性能而已。但事实证明，多个线程同时修改一个对象的场景才是少数，绝大多数可变对象只会被一个线程修改，然而因为频繁不必要的上锁、释放锁的操作，使得性能损失很大，所以你会看到，后来的 ArrayList、HashMap 等可变类都不再默认线程安全，以此换取单线程下的性能提升，然后用 Collections.synchronizedXxx 等方法提供少数情况需要多线程安全的可变对象。 这个问题的主角 StringBuffer 和 StringBuilder 也是一样的，后者就是为纠正历史遗留问题，提高单线程下的性能而生的，而为了向下兼容性和需要线程安全的场景，才保留 StringBuffer 类。 随着内存越来越大，函数式编程的“万物皆 immutable”的思想流行，Java 也出现越来越多的不可变类，例如 java.time 包中的类、Optional 类、 List.of、Set.of 返回的不可变集合等。这才是多进程、多线程下保证准确性、提升性能的新的解决方案。 四个线程安全策略 线程限制1一个被线程限制的对象，由线程独占，只能被线程占有的线程修改 共享只读1在没有额外同步的情况下，可以被多个线程并发访问，不能修改 线程安全对象1一个线程安全的对象或者容器，在内部通过同步机制来保证线程安全，所以其他线程无需额外的同步就可以通过公共接口随意访问它 被守护对象1被守护对象只能通过获取特定的锁来访问 不可变对象在Java中，有一种对象发布了就是安全的，被称之为不可变对象。 不可变对象可以在多线程中可以保证线程安全 不可变对象满足的条件 对象创建之后状态就不能修改 对象的域都是 final 类型 对象是正确创建的（对象在创建期间，this引用没有溢出，外部不可见） 创建不可变对象 将类声明成final类型，使其不可以被继承 将所有的成员设置成私有的，使其他的类和对象不能直接访问这些成员 对变量不提供set方法 将所有可变的成员声明为final，这样只能对他们赋值一次 通过构造器初始化所有成员，进行深度拷贝 在get方法中，不直接返回对象本身，而是克隆对象，返回对象的拷贝 JDK中的 Collections.unmodifiableXXX传入的对象一经初始化便无法修改,XXX可表示Collection、List、Set、Map等 Collections.UnmodifiableMap的源码 主要是将一个新的集合所有更新方法变成抛出异常","link":"/2022/04/18/%E5%85%B6%E4%BB%96/String/String/"},{"title":"kaptcha验证码","text":"kaptcha 验证码框架 1. 导入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.github.penggle&lt;/groupId&gt; &lt;artifactId&gt;kaptcha&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt;&lt;/dependency&gt; 2. 简单使用首先指的生成的验证码类型 text or math 注册为 bean 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Configurationpublic class CaptchaConfig { @Bean(name = &quot;captchaProducer&quot;) public DefaultKaptcha getKaptchaBean() { DefaultKaptcha defaultKaptcha = new DefaultKaptcha(); Properties properties = new Properties(); // 是否有边框 默认为true 我们可以自己设置yes，no properties.setProperty(KAPTCHA_BORDER, &quot;yes&quot;); // 验证码文本字符颜色 默认为Color.BLACK properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_COLOR, &quot;black&quot;); // 验证码图片宽度 默认为200 properties.setProperty(KAPTCHA_IMAGE_WIDTH, &quot;160&quot;); // 验证码图片高度 默认为50 properties.setProperty(KAPTCHA_IMAGE_HEIGHT, &quot;60&quot;); // 验证码文本字符大小 默认为40 properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_SIZE, &quot;38&quot;); // KAPTCHA_SESSION_KEY properties.setProperty(KAPTCHA_SESSION_CONFIG_KEY, &quot;kaptchaCode&quot;); // 验证码文本字符长度 默认为5 properties.setProperty(KAPTCHA_TEXTPRODUCER_CHAR_LENGTH, &quot;4&quot;); // 验证码文本字体样式 默认为new Font(&quot;Arial&quot;, 1, fontSize), new Font(&quot;Courier&quot;, 1, fontSize) properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_NAMES, &quot;Arial,Courier&quot;); // 图片样式 水纹com.google.code.kaptcha.impl.WaterRipple 鱼眼com.google.code.kaptcha.impl.FishEyeGimpy 阴影com.google.code.kaptcha.impl.ShadowGimpy properties.setProperty(KAPTCHA_OBSCURIFICATOR_IMPL, &quot;com.google.code.kaptcha.impl.ShadowGimpy&quot;); Config config = new Config(properties); defaultKaptcha.setConfig(config); return defaultKaptcha; } @Bean(name = &quot;captchaProducerMath&quot;) public DefaultKaptcha getKaptchaBeanMath() { DefaultKaptcha defaultKaptcha = new DefaultKaptcha(); Properties properties = new Properties(); // 是否有边框 默认为true 我们可以自己设置yes，no properties.setProperty(KAPTCHA_BORDER, &quot;yes&quot;); // 边框颜色 默认为Color.BLACK properties.setProperty(KAPTCHA_BORDER_COLOR, &quot;105,179,90&quot;); // 验证码文本字符颜色 默认为Color.BLACK properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_COLOR, &quot;blue&quot;); // 验证码图片宽度 默认为200 properties.setProperty(KAPTCHA_IMAGE_WIDTH, &quot;160&quot;); // 验证码图片高度 默认为50 properties.setProperty(KAPTCHA_IMAGE_HEIGHT, &quot;60&quot;); // 验证码文本字符大小 默认为40 properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_SIZE, &quot;35&quot;); // KAPTCHA_SESSION_KEY properties.setProperty(KAPTCHA_SESSION_CONFIG_KEY, &quot;kaptchaCodeMath&quot;); // 验证码文本生成器// properties.setProperty(KAPTCHA_TEXTPRODUCER_IMPL, &quot;com.ruoyi.framework.config.KaptchaTextCreator&quot;); // 验证码文本字符间距 默认为2 properties.setProperty(KAPTCHA_TEXTPRODUCER_CHAR_SPACE, &quot;3&quot;); // 验证码文本字符长度 默认为5 properties.setProperty(KAPTCHA_TEXTPRODUCER_CHAR_LENGTH, &quot;6&quot;); // 验证码文本字体样式 默认为new Font(&quot;Arial&quot;, 1, fontSize), new Font(&quot;Courier&quot;, 1, fontSize) properties.setProperty(KAPTCHA_TEXTPRODUCER_FONT_NAMES, &quot;Arial,Courier&quot;); // 验证码噪点颜色 默认为Color.BLACK properties.setProperty(KAPTCHA_NOISE_COLOR, &quot;white&quot;); // 干扰实现类 properties.setProperty(KAPTCHA_NOISE_IMPL, &quot;com.google.code.kaptcha.impl.NoNoise&quot;); // 图片样式 水纹com.google.code.kaptcha.impl.WaterRipple 鱼眼com.google.code.kaptcha.impl.FishEyeGimpy 阴影com.google.code.kaptcha.impl.ShadowGimpy properties.setProperty(KAPTCHA_OBSCURIFICATOR_IMPL, &quot;com.google.code.kaptcha.impl.ShadowGimpy&quot;); Config config = new Config(properties); defaultKaptcha.setConfig(config); return defaultKaptcha; }} 3. 控制层代码12345678910111213141516171819202122@RestControllerpublic class CaptchaController { @Autowired @Qualifier(&quot;captchaProducer&quot;) DefaultKaptcha defaultKaptcha; @Autowired @Qualifier(&quot;captchaProducerMath&quot;) DefaultKaptcha defaultKaptchaMath; @GetMapping(&quot;/img&quot;) public void captchaImg(HttpServletResponse response) throws IOException { String text = defaultKaptcha.createText(); BufferedImage image = defaultKaptcha.createImage(text); ImageIO.write(image,&quot;jpg&quot;,response.getOutputStream()); } @GetMapping(&quot;/math&quot;) public void captchaMath(HttpServletResponse response) throws IOException { String text = defaultKaptchaMath.createText(); BufferedImage image = defaultKaptchaMath.createImage(text); ImageIO.write(image,&quot;jpg&quot;,response.getOutputStream()); } 4. 测试测试 img 测试math 项目demo ：https://gitee.com/sahura/kaptcha.git","link":"/2022/05/23/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/%E9%AA%8C%E8%AF%81%E7%A0%81/kaptcha/"},{"title":"固定ip","text":"1.1 检查虚拟机 虚拟网络编辑器 1.2 检查宿主机设置手动获取IP地址 1.3 进入 bash 修改 ip 信息1vi /etc/sysconfig/network-scipts/ifcfg-ens33 具体修改内容为 12345678910111213141516171819202122TYPE=EthernetPROXY_METHOD=noneBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=15583e19-93f8-4939-9a0e-4200b57d99e7DEVICE=ens33ONBOOT=yesDNS1=144.144.144.144IPV6_PRIVACY=noPROXY_METHOD=noneBROWSER_ONLY=no# 新增以下IPADDR=192.168.170.129GATEWAY=192.168.170.2NETMASK=255.255.255.0 1.4 重启网络配置1systemctl restart network 1.5 检查效果1ip address 1.6 特殊问题配置都是正常 就是无法联网 解决方式：禁用NetworkManager 重启 network 解决 123456systemctl stop NetworkManagersystemctl disable NetworkManagersystemctl restart network","link":"/2022/05/12/%E5%85%B6%E4%BB%96/linux%E5%91%BD%E4%BB%A4/centos7%20%E8%AE%BE%E7%BD%AE%E5%9B%BA%E5%AE%9Aip/"},{"title":"linux指令","text":"查询 java 进程状态1ps -ef | grep java 查询 java 安装目录123ls -lrt /usr/bin/java##ls -lrt /etc/alternatives/java 查看 java 进程状态1jps 查询 cpu 使用最高1ps aux|head -1;ps aux|grep -v PID|sort -rn -k +3|head 查看占用内存最高的进程1ps aux|head -1;ps aux|grep -v PID|sort -rn -k +4|head","link":"/2022/05/13/%E5%85%B6%E4%BB%96/linux%E5%91%BD%E4%BB%A4/linux%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"title":"docker","text":"1. yum 安装 gcc 相关查看 gcc 版本 1gcc -v 需要安装的话 安装 gcc 12yum -y install gccyum -y install gcc-c++ 2. 安装需要的软件包1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 3. 设置 stable 镜像库12# 官方推荐sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 4. 更新 yum 软件包 索引1yum makecache fast 5. 安装 docker ce1234# 官网sudo yum -y install docker-ce docker-ce-cli containerd.io#yum -y install docker-ce 6. 测试12345#查询版本docker version#启动dockersystemctl start docker 7. 配置加速1234567891011sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json# daemon.json中的内容{&quot;registry-mirrors&quot;: [&quot;https://eu2u8bif.mirror.aliyuncs.com&quot;]}# 重新加载daemon.jsonsudo systemctl daemon-reload# 重启dockersudo systemctl restart docker 8. 卸载123456# 停止dockersystemctl stop docker# 移除dockersudo yum -y remove docker-ce# 移除所有容器/镜像/数据卷sudo rm -rf /var/lib/docker","link":"/2022/05/15/%E5%85%B6%E4%BB%96/linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/docker%E5%AE%89%E8%A3%85/"},{"title":"StringJoiner","text":"一、StringJoinerStringJoiner是Java8新出的一个类，用于构造由分隔符分隔的字符序列，并可选择性地从提供的前缀开始和以提供的后缀结尾 123456789101112@Testvoid contextLoads() { // 参数 1，元素分隔符；2 前缀 3 后缀 StringJoiner joiner = new StringJoiner(&quot;,&quot;,&quot;[&quot;,&quot;]&quot;); // 里面没有元素时，会返回默认值 joiner.setEmptyValue(&quot;我是默认值&quot;); System.out.println(joiner.toString()); for (int i = 0; i &lt; 10; i++) { joiner.add(i+&quot;&quot;); } System.out.println(joiner.toString());} 结果 12我是默认值[0,1,2,3,4,5,6,7,8,9] 二、String.joinStringJoiner的问题在于，您需要知道有一个StringJoiner类，它并不算太坏，但是如果您可以直接从非常流行的java.lang.String类本身中加入String，该怎么办呢？ 嗯，这就是Java设计师的想法，他们添加了一个静态join（）方法来直接从String类本身连接String。 12345@Testvoid contextLoads1() { String colonSeparatedValue = String.join(&quot;:&quot;, &quot;abc&quot;, &quot;bcd&quot;, &quot;def&quot;); System.out.println(&quot;colon separated String : &quot; + colonSeparatedValue);} 输出结果 1colon separated String : abc:bcd:def JAVA生成 CSV String;可以在Java中将 字符串列表直接转换为CSV字符串 ，而无需编写任何手动代码，下面是一个示例。 123456@Testvoid contextLoads3() { List mylist = Arrays.asList(&quot;London&quot;, &quot;Paris&quot;, &quot;NewYork&quot;); String joined = String.join(&quot;||&quot;, mylist); System.out.println(&quot;Joined String : &quot; + joined);} 输出 1Joined String : London||Paris||NewYork 无需再用Java手动将字符串列表或一组字符串转换为CSV字符串。 还值得注意的是，==String.join（）内部使用StringJoiner类来连接String文字==。","link":"/2022/05/05/%E5%85%B6%E4%BB%96/String/StringJoiner/"},{"title":"firewalld","text":"firewalld1.1 基本使用：启动： 1systemctl start firewalld 关闭： 1systemctl stop firewalld 查看状态： 1systemctl status firewalld 开机禁用 ： 1systemctl disable firewalld 开机启用 ： 1systemctl enable firewalld 1.2 配置 firewalld查看版本： 1firewall-cmd --version 查看帮助： 1firewall-cmd --help 显示状态： 1firewall-cmd --state 查看所有打开的端口： 1firewall-cmd --list-ports 更新防火墙规则： 每次修改都要执行 1firewall-cmd --reload 那怎么开启一个端口呢（–permanent永久生效，没有此参数重启后失效） 1firewall-cmd --zone=public --add-port=8927/tcp --permanent 查看规则 1firewall-cmd --zone=public --query-port=80/tcp 删除规则 1firewall-cmd --zone=public --remove-port=80/tcp--permanent 调整默认策略（默认拒绝所有访问，改成允许所有访问）： 1firewall-cmd --permanent --zone=public --set-target=ACCEPT 对某个IP开放多个端口： 12firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;10.159.60.29&quot; port protocol=&quot;tcp&quot; port=&quot;1:65535&quot; accept&quot;firewall-cmd --reload","link":"/2022/05/13/%E5%85%B6%E4%BB%96/linux%E5%91%BD%E4%BB%A4/%E9%98%B2%E7%81%AB%E5%A2%99%E7%9B%B8%E5%85%B3/"},{"title":"gitlab","text":"GitLab 下载地址 : https://packages.gitlab.com/gitlab 安装基础依赖 1yum install -y curl policycoreutils-python openssh-server docker 安装 2.1 下载 中文 镜像版本1docker pull beginor/gitlab-ce:11.3.0-ce.0 2.2 创建目录通常会将 GitLab 的配置 (etc) 、 日志 (log) 、数据 (data) 放到容器之外， 便于日后升级， 因此请先准备这三个目录。 12345mkdir -p /mydata/gitlab/etcmkdir -p /mydata/gitlab/logmkdir -p /mydata/gitlab/data 赋予权限 12345chmod 777 /mydata/gitlab/etcchmod 777 /mydata/gitlab/logchmod 777 /mydata/gitlab/data 2.3 运行镜像123456789sudo docker run --detach \\ --hostname gitlab.example.com \\ --publish 5443:443 --publish 84:84 --publish 222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/mydata/gitlab/etc \\ --volume /srv/gitlab/logs:/mydata/gitlab/log \\ --volume /srv/gitlab/data:/mydata/gitlab/data \\ --privileged=true beginor/gitlab-ce:11.3.0-ce.0 命令解释 12345678910sudo docker run --detach \\ --hostname gitlab.example.com \\ # 设置主机名或域名 --publish 5443:443 --publish 84:84 --publish 222:22 \\ # 本地端口的映射 将本地 433映射到外部 5443 --name gitlab \\ # gitlab-ce 的镜像运行成为一个容器，这里是对容器的命名 --restart always \\ # 设置重启方式，always 代表一直开启，服务器开机后也会自动开启的 --volume /srv/gitlab/config:/mydata/gitlab/etc \\ # 将 gitlab 的配置文件目录映射到 /mydata/gitlab/etc 目录中 --volume /srv/gitlab/logs:/mydata/gitlab/log \\ # 将 gitlab 的log文件目录映射到 /mydata/gitlab/log 目录中 --volume /srv/gitlab/data:/mydata/gitlab/data \\ # 将 gitlab 的数据文件目录映射到 /mydata/gitlab/data 目录中 beginor/gitlab-ce:11.3.0-ce.0 # 需要运行的镜像 2.4 检查1234567891011121314151617181920212223#查看容器列表docker ps -a#查看容器启动列表docker ps # 开启 gitlab 容器# 上面那个特别长的那一条命令仅在第一次运行 gitlab 的时候需要# 之后开启 gitlab 使用下面的命令即可 sudo docker start gitlab# 关闭 gitlab sudo docker stop gitlab# 重启 gitlab sudo docker restart gitlab //进入容器命令行docker exec -it gitlab bash//容器中应用配置，让修改后的配置生效gitlab-ctl reconfigure//容器中重启服务gitlab-ctl restart","link":"/2022/05/16/%E5%85%B6%E4%BB%96/linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/gitlab%E5%AE%89%E8%A3%85/"},{"title":"springboot 集成 MongoDB","text":"前言非关系性数据库 mongoDB 学习记录 1. mongoDB 由于MongoDB独特的数据处理方式，可以将热点数据加载到内存，故而对查询来讲，会非常快（当然也会非常消耗内存）；同时由于采用了BSON的方式存储数据，故而对JSON格式数据具有非常好的支持性以及友好的表结构修改性，文档式的存储方式，数据友好可见；数据库的分片集群负载具有非常好的扩展性以及非常不错的自动故障转移。 缺点：锁只能到 collection 级别，做不到行级别锁，存在大量复杂事务逻辑，不要使用 MongoDB 具体区别 1 mongoDB MySQL 数据库模型 非关系型 关系型 存储方式 虚拟内存+持久化 不同引擎有不同的存储方式 查询语句 mogodb独有查询方式 传统sql 架构特点 可以通过副本集，分片的方式实现高可用 单点，M-S 等架构 数据处理方式 基于内存，热数据存在物理内存，实现高度读写 不同引擎有不同的存储方式 具体区别 2 sql术语/概念 mongodb术语/概念 说明/解释 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接，mongodb不支持 primary key primary key 主键，mongodb 自动 _id 设置为主键 2. spring boot 集成 2.1 添加依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 2.2 配置 mongodb123456789101112131415spring.data.mongodb.uri=mongodb://localhost:27017/test# 设置了密码的MongoDB配置方式# MongoDB服务器连接地址#spring.data.mongodb.host=127.0.0.1# MongoDB服务器连接端口#spring.data.mongodb.port=27017# MongoDB的验证数据库#spring.data.mongodb.authentication-database=admin# MongoDB数据库用户#spring.data.mongodb.username=root# MongoDB数据库密码#spring.data.mongodb.password=123456# 带连接的数据库#spring.data.mongodb.database=test 2.3 entity12345678910111213141516171819202122import lombok.Data;@Datapublic class Hobbies { private String hname;}import lombok.Data;import org.bson.codecs.pojo.annotations.BsonId;@Datapublic class Student { @BsonId private String id; private String name; private Integer age; private Integer sex; private Integer height; private Hobbies hobbies;} 2.4 dao常规的增删改查 12345678910111213141516171819202122232425262728293031323334import com.example.mongodb.entity.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.core.query.Update;import org.springframework.stereotype.Repository;import java.util.List;@Repositorypublic class StudentDao { @Autowired MongoTemplate mongoTemplate; // 增 public void save(Student s) { mongoTemplate.save(s); } // 查 public Student get(Query query) { return mongoTemplate.findOne(query,Student.class); } // 查 public List&lt;Student&gt; findAll() { return mongoTemplate.findAll(Student.class); } // 改 public void update(Query query, Update update) { mongoTemplate.updateMulti(query,update,Student.class); } // 删 public void delete(Query query) { mongoTemplate.remove(query,Student.class); }} 2.5 service1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import com.example.mongodb.dao.StudentDao;import com.example.mongodb.entity.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.core.query.Update;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class StudentService { @Autowired StudentDao studentDao; public void save(Student student) { studentDao.save(student); } public Student get() { //and查询 /* Criteria criteriaName=Criteria.where(&quot;name&quot;).is(&quot;lisi&quot;); Criteria criteriaAage=Criteria.where(&quot;age&quot;).is(17); Criteria andCriteria = new Criteria(); andCriteria.andOperator(criteriaName,criteriaAage); Query query=new Query(andCriteria);*/ //or查询 Criteria criteriaName=Criteria.where(&quot;name&quot;).is(&quot;lisi&quot;); Criteria criteriaAage=Criteria.where(&quot;age&quot;).gt(16); Criteria orCriteria = new Criteria(); orCriteria.orOperator(criteriaName,criteriaAage); Query query=new Query(orCriteria); return studentDao.get(query); } public List&lt;Student&gt; findAll() { return studentDao.findAll(); } public void update() { Query query=new Query(Criteria.where(&quot;name&quot;).is(&quot;zhangsan&quot;)); Update update=new Update(); update.set(&quot;age&quot;,30); update.set(&quot;height&quot;,188); update.set(&quot;hobbies.hname&quot;,&quot;basketball&quot;); studentDao.update(query,update); } public void delete() { Query query=new Query(Criteria.where(&quot;name&quot;).is(&quot;zhangsan&quot;)); studentDao.delete(query); }} 2.6 test123456789101112131415161718192021222324252627282930@SpringBootTestclass MongodbApplicationTests { @Autowired StudentService studentService; @Autowired ObjectMapper objectMapper; @Test void mongodbSave() { // mongodb save 数据 Student s=new Student(); s.setName(&quot;张三&quot;); s.setAge(17); s.setSex(1); s.setHeight(182); Hobbies h=new Hobbies(); h.setHname(&quot;eating&quot;); s.setHobbies(h); studentService.save(s); } @SneakyThrows @Test void mongodbGet() { // mongodb get 数据 final Student student = studentService.get(); System.out.println(objectMapper.writeValueAsString(student)); } /** * {&quot;id&quot;:&quot;6285e5f178157f2887b505ce&quot;,&quot;name&quot;:&quot;lis&quot;,&quot;age&quot;:17,&quot;sex&quot;:1,&quot;height&quot;:182,&quot;hobbies&quot;:{&quot;hname&quot;:&quot;swing&quot;}} */ 数据库实际数据 demo 地址：https://gitee.com/sahura/mongodb.git","link":"/2022/05/21/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/springBoot%E9%9B%86%E6%88%90mongodb/"},{"title":"MySQL 之 三大日志","text":"1.binlogbinlog 用来记录数据库执行的写入操作（不包含查询）信息，以二进制的形式保存在磁盘中。binlog是mysql的逻辑日志，并且 由 Server 层记录日志。 逻辑日志：可以简单理解为记录的 sql 语句 物理日志：mysql数据最终保存在数据页中，物理日志记录就是数据页的变更 binlog 是通过追加的方式进行写入的。可以通过 max_binlog_size 设定大小。达到预定值，会重新生成新的 binlog 保存日志 1.1 使用场景 主从复制：在master 端开启binlog，然后通过 binlog 发送到 slave 端， salve 通过重放 binlog 达到 主从数据一致 数据恢复：通过 mysqlbinlog 工具来恢复数据 2.redolog事务的四大特性 持久性 ：具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。那么mysql是如何保证一致性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：1） 因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！2）一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。 3.undolog数据库事务四大特性中有一个是原子性，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。实际上，原子性底层就是通过undo log实现的。undo log主要记录了数据的逻辑变化，比如一条INSERT语句，对应一条DELETE的undo log，对于每个UPDATE语句，对应一条相反的UPDATE的undo log，这样在发生错误时，就能回滚到事务之前的数据状态。","link":"/2022/06/01/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/mysql-%E4%B8%89%E5%A4%A7%E6%97%A5%E5%BF%97/"},{"title":"MySQL 之 redolog","text":"1. 谁的 redo log学习 redo log，我觉得首先要搞明白一个问题，就是是谁的 redo log？ 我们知道，MySQL 架构整体上分为两层：Server 层和存储引擎层，如下图： 前面松哥文章+视频跟大家聊的 binlog，是 MySQL 自己提供的 binlog，而 redo log 则不是 MySQL 提供的，而是存储引擎 InnoDB 自己提供的。所以在 MySQL 中就存在两类日志 binlog 和 redo log，存在两类日志既有历史原因（InnoDB 最早不是 MySQL 官方存储引擎）也有技术原因，这个咱们以后再细聊。 先把这个问题搞清楚，后面很多地方就容易懂了。 2. buffer pool在正式介绍 redo log 之前，还有一个 buffer pool 需要大家了解。 小伙伴们知道，InnoDB 引擎存储数据的时候，是以页为单位的，每个数据页的大小默认是 16KB，我们可以通过如下命令来查看页的大小： 116384/1024=16 刚好是 16KB。 计算机在存储数据的时候，最小存储单元是扇区，一个扇区的大小是 512 字节，而文件系统（例如 XFS/EXT4）最小单元是块，一个块的大小是 4KB，也就是四个块组成一个 InnoDB 中的页。我们在 MySQL 中针对数据库的增删改查操作，都是操作数据页，说白了，就是操作磁盘。 但是大家想想，如果每一次操作都操作磁盘，那么就会产生海量的磁盘 IO 操作，如果是传统的机械硬盘，还会涉及到很多随机 IO 操作，效率低的令人发指。这严重影响了 MySQL 的性能。 为了解决这一问题，MySQL 引入了 buffer pool，也就是我们常说的缓冲池。 buffer pool 的主要作用就是缓存索引和表数据，以避免每一次操作都要进行磁盘 IO，通过 buffer pool 可以提高数据的访问速度。 通过如下命令可以查看 buffer pool 的默认大小： 1134217728/1024/1024=128 默认大小是 128MB，因为松哥这里的 MySQL 是安装在 Docker 中，所以这个分配的小一些。一般来说，如果一个服务器只是运行了一个 MySQL 服务，我们可以设置 buffer pool 的大小为服务器内存大小的 75%～80%。 3. change buffer在正式介绍 redo log 之前，还有一个 change buffer 需要大家了解。 前面我们说的 buffer pool 虽然提高了访问速度，但是增删改的效率并没有因此提升，当涉及到增删改的时候，还是需要磁盘 IO，那么效率一样低的令人发指。 为了解决这个问题，MySQL 中引入了 change buffer。change buffer 以前并不叫这个名字，以前叫 insert buffer，即只针对 insert 操作有效，现在改名叫 change buffer 了，不仅仅针对 insert 有效，对 delete 和 update 操作也是有效的，change buffer 主要是对非唯一的索引有效，如果字段是唯一性索引，那么更新的时候要去检查唯一性，依然无法避免磁盘 IO。 change buffer 就是说，当我们需要更改数据库中的数据的时候，我们把更改记录到内存中，等到将来数据被读取的时候，再将内存中的数据 merge 到 buffer pool 然后返回，此时 buffer pool 中的数据和磁盘中的数据就会有差异，有差异的数据我们称之为脏页，在满足条件的时候（redo log 写满了、内存写满了、其他空闲时候），InnoDB 会把脏页刷新回磁盘。这种方式可以有效降低写操作的磁盘 IO，提升数据库的性能。 通过如下命令我们可以查看 change buffer 的大小以及哪些操作会涉及到 change buffer： innodb_change_buffer_max_size：这个配置表示 change buffer 的大小占整个缓冲池的比例，默认值是 25%，最大值是 50%。 innodb_change_buffering：这个操作表示哪些写操作会用到 change buffer，默认的 all 表示所有写操作，我们也可以自己设置为 none/inserts/deletes/changes/purges 等。 不过 change buffer 和 buffer pool 都涉及到内存操作，数据不能持久化，那么，当存在脏页的时候，MySQL 如果突然挂了，就有可能造成数据丢失（因为内存中的数据还没写到磁盘上），但是我们在实际使用 MySQL 的时候，其实并不会有这个问题，那么问题是怎么解决的？那就得靠 redo log 了。 4. redo log 的诞生在正式介绍 redo log 之前，还需要给大家普及一个概念：WAL。 WAL 全称是 Write-Ahead Logging 中文译作预写日志。啥意思呢？就是说 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上，这样的好处是错开高峰期的磁盘 IO，提高 MySQL 的性能。 配合上前面的 buffer pool 和 change buffer，WAL 就是说在操作 buffer pool 和 change buffer 之前，会先把记录写到 redo log 日志中，然后再去更新 buffer pool 或者 change buffer，这样，即使系统突然崩了，将来也可以通过 redo log 恢复数据。当然，redo log 本身又分为： 日志缓冲（redo log buffer)，该部分日志是易失性的。 重做日志(redo log file)，这是磁盘上的日志文件，该部分日志是持久的。 那有人说，写 redo log 不就是磁盘 IO 吗？而写数据到磁盘也是磁盘 IO，既然都是磁盘 IO，那干嘛不把直接把数据写到磁盘呢？还费这事！ 此言差矣。 写 redo log 跟写数据有一个很大的差异，那就是 redo log 是顺序 IO，而写数据涉及到随机 IO，写数据需要寻址，找到对应的位置，然后更新/添加/删除，而写 redo log 则是在一个固定的位置循环写入，是顺序 IO，所以速度要高于写数据。 如前文所说，redo log 涉及到两个东西：redo log buffer 和 redo log file，这两个东西我们分别来介绍。 4.1 redo log buffer先来说 redo log buffer。 我们说数据的变化先写入 redo log 中，并不是上来就写磁盘，也是先写到内存中，即 redo log buffer，在时机成熟时，再写入磁盘，也就是 redo log file。 我们先来看看 redo log buffer 有多大： 116777216 ÷ 1024 ÷ 1024 = 16MB 可以看到，这个 redo log buffer 大小刚好是 16MB，如果你觉得这个值有点小，也可以自行修改其大小。 数据的变更都会首先记录在这块内存中。小伙伴们知道，MySQL 的增删改，如果我们没有显式的开启事务，MySQL 内部也是有一个事务存在的，当内部这个事务 commit 的时候，redo log buffer 会持久化到磁盘中。 具体来说，有如下几个持久化时机： innodb_flush_log_at_trx_commit 通过 innodb_flush_log_at_trx_commit 参数来控制持久化时机，该参数默认值为 1，如下图： 当然开发者可根据自己的实际需求修改该参数。该参数有三种取值，含义分别如下： 0：每秒一次，将 redo log buffer 中的数据刷新到磁盘中。 1：每次 commit 时，将 redo log buffer 中的数据刷新到磁盘中，即只要 commit 成功，磁盘上就有对应的 redo log 日志，这是最安全的情况，也是推荐使用的参数。 2：每次 commit 时，将 redo log buffer 中的数据刷新到操作系统缓存中，操作系统缓存中的数据每秒刷新一次，会持久化到磁盘中。 这是第一种 redo log buffer 持久化的时机。 当 redo log buffer 的使用量达到 innodb_log_buffer_size 的一半时，将其写入磁盘成为 redo log file。 MySQL 关闭时，将 redo log buffer 写入磁盘成为 redo log file。 那如果 redo log buffer 中的数据还没有磁盘，MySQL 就挂了该怎么办？没写入磁盘，说明你还没 commit，既然没 commit，那就数据修改操作都还没有完成，那只能丢了就丢了，如果已经 commit 了，那么数据就会持久化到 redo log file 中，此时即使 MySQL 挂了，将来 MySQL 重启恢复了，数据也是可以被恢复的。具体的恢复逻辑，就涉及到两阶段提交了，这个松哥在后面的文章中再和大家详细介绍。 4.2 redo log 落盘还有一个需要大家注意的问题就是 redo log 落盘，落盘的数据从哪里来？是从 redo log 日志中来还是从 buffer pool 中来？ 在前面的文章中我们说过：binlog 是一种逻辑日志，他里边所记录的是一条 SQL 语句的原始逻辑，例如给某一个字段 +1，这区别于 redo log 的物理日志，物理日志记录的是在某个数据页上做了什么修改。 由于 redo log 并没有记录数据页的完整数据，所以正常的落盘其实用不到 redo log，数据落盘的时机到了时，直接拿着将脏页（buffer pool）持久化到磁盘中即可","link":"/2022/06/01/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/mysql-redolog/"},{"title":"jenkins","text":"一、介绍1.1 持续集成流程说明 首先开发人员代码提交，提交到 Git 仓库 然后 Jenkins 作为持续集成工具，使用 Git 工具到 Git 仓库到集成服务器，配合 JDK,Maven 等工具完成代码编译，代码测试与审查，打包等工作。每一个流程出差，都会重新执行一次整个流程 最后，jenkins 把生成的 jar 或者 war 包分发到测试服务器或者生产服务器。测试人员或者用户就可以访问这个应用 1.2 安装1.2.1 环境准备 JDK 环境 java -version Maven 环境 mvn -version 1.2.2 启动 Jenkins 库官网地址：https://www.jenkins.io/ 清华大学镜像源：https://mirrors.tuna.tsinghua.edu.cn/jenkins/redhat-stable/ 1wget https://mirrors.tuna.tsinghua.edu.cn/jenkins/redhat-stable/jenkins-2.332.2-1.1.noarch.rpm --no-check-certificate 1.2.3 安装 Jenkins1yum install jenkins-2.332.2-1.1.noarch.rpm -y // 安装 1.2.4 修改默认端口12vim /etc/sysconfig/jenkins 修改 jenkins 权限 为 root 配置端口，默认：JENKINS_PORT=”8080”,修改为 18080 1.2.5 启动1234systemctl start jenkinssystemctl status jenkinssystemctl stop jenkins 启动成功","link":"/2022/05/17/%E5%85%B6%E4%BB%96/linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/jenkins/"},{"title":"maven","text":"1. yum 方式安装12yum -y install maven 看看是否安装成功 12345678mvn -versionApache Maven 3.0.5 (Red Hat 3.0.5-17)Maven home: /usr/share/mavenJava version: 1.8.0_201, vendor: Oracle CorporationJava home: /usr/java/jdk1.8.0_201-amd64/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: “linux”, version: “3.10.0-957.el7.x86_64”, arch: “amd64”, family: “unix” 存在问题：yum 方式安装 版本过低, 有的插件不支持低版本 maven 2. 手动 安装从官网获取maven下载地址https://maven.apache.org/download.cgi，右键复制链接地址 下载文件 上传服务器 解压 1tar zxvf apache-maven-3.8.5-bin.tar.gz 配置环境变量 1vi /etc/profile 按Shitf+G进入最后一行，然后按字母i进入编辑模式，插入以下数据，按Esc，输入:wq回车 MAVEN_HOME=/usr/local/maven/apache-maven-x.x.xexport PATH=${MAVEN_HOME}/bin:${PATH} 重载环境变量 1source /etc/profile 查看安装结果 1mvn -v 安装成功 配置 maven 12vim /usr/local/maven/apache-maven-3.8.5/conf/settings.xml 修改镜像 Maven镜像仓库替换为阿里云镜像仓库，关键字mirror 1234567&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;","link":"/2022/05/20/%E5%85%B6%E4%BB%96/linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/maven%20%E5%AE%89%E8%A3%85/"},{"title":"分布式事务 之 seata","text":"1. 什么是反向补偿首先，来和大家解释一个名词，大家在看分布式事务相关资料的时候，经常会看到一个名词：反向补偿。啥是反向补偿呢？ 我举一个例子：假设我们现在有三个微服务分别是 A、B、C，现在在 A 服务中分别调用 B 和 C 服务，为了确保 B 和 C 同时成功或者同时失败，我们需要使用到分布式事务。但是按照我们之前对本地事务的理解，B 和 C 中的本地事务，当 B 服务中的事务执行完毕并且提交之后，现在 C 服务中的事务出现异常需要回滚了，但是，B 已经提交了还怎么回滚呀？ 此时我们所说的回滚其实并不是传统意义上的，通过 MySQL redo log 日志来回滚的那种，而是通过一条更新 SQL，再把 B 服务中已经更改过的数据复原。 这就是我们所说的反向补偿！ 2. 基本概念梳理Seata 中有三个核心概念： TC (Transaction Coordinator) - 事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚。 TM (Transaction Manager) - 事务管理器：定义全局事务的范围，开始全局事务、提交或回滚全局事务。 RM ( Resource Manager ) - 资源管理器：管理分支事务处理的资源( Resource )，与 TC 交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 其中，TC 为单独部署的 Server 服务端，TM 和 RM 为嵌入到应用中的 Client 客户端。我们来看如下一张图片： 这张图基本上把这个三个概念解释清楚了。 其实不看这张图，我们大概也能猜到分布式事务的实现原理：首先得有一个全局的事务协调者（TC），各个本地事务（RM）在开始执行的时候，或者在执行的过程中，及时将自己的状态报告给全局事务协调者，这个全局事务协调者知道每一个分支事务目前的执行状态，当他（TC）发现所有的本地事务都执行成功的时候，就通知大家一起提交；当他发现在本次事务中，有人执行失败的时候，就通知大家一起回滚（当然这个回滚不一定是真的回滚，而是反向补偿）。那么一个事务什么时候开始什么时候结束呢？也就是事务的边界在哪里？seata 中的分布式事务都是通过 @GlobalTransactional 注解来实现的，换句话说，这个注解该加在哪里？添加该注解的地方其实就是事务管理器 TM 了。 经过上面的介绍，小伙伴们应该明白了，其实用 Seata 实现分布式事务没有想象的那么难，原理还是非常 Easy 的。 Seata 中涉及到四种不同的模式，接下来介绍的四种不同模式，其实都是在说当有一个本地事务失败的时候，该如何回滚？这就是我们后面要说的四种不同的分布式事务模式了。 3. 什么是两阶段提交先来看下面一张图： 这张图里涉及到三个概念： AP：这个不用多说，AP 就是应用程序本身。 RM：RM 是资源管理器，也就是事务的参与者，大部分情况下就是指数据库，一个分布式事务往往涉及到多个 RM。 TM：TM 就是事务管理器，创建分布式事务并协调分布式事务中的各个子事务的执行和状态，子事务就是指在 RM 上执行的具体操作。 那么什么是两阶段(Two-Phase Commit, 简称 2PC)提交？ 两阶段提交说白了道理很简单，松哥举个简单例子来和大家说明两阶段提交： 比如下面一张图： 我们在 Business 中分别调用 Storage 与 Order、Account，这三个中的操作要同时成功或者同时失败，但是由于这三个分处于不同服务，因此我们只能先让这三个服务中的操作各自执行，三个服务中的事务各自执行就是两阶段中的第一阶段。 第一阶段执行完毕后，先不要急着提交，因为三个服务中有的可能执行失败了，此时需要三个服务各自把自己一阶段的执行结果报告给一个事务协调者，事务协调者收到消息后，如果三个服务的一阶段都执行成功了，此时就通知三个事务分别提交，如果三个服务中有服务执行失败了，此时就通知三个事务分别回滚。 这就是所谓的两阶段提交。 总结一下：两阶段提交中，事务分为参与者（例如上图的各个具体服务）与协调者，参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是要提交操作还是中止操作，这里的参与者可以理解为 RM，协调者可以理解为 TM。 不过 Seata 中的各个分布式事务模式，基本都是在二阶段提交的基础上演化出来的，因此并不完全一样，这点需要小伙伴们注意。 4. AT 模式AT 模式是一种全自动的事务回滚模式。 整体上来说，AT 模式是两阶段提交协议的演变： 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段则分两种情况：2.1 提交异步化，非常快速地完成。2.2 回滚通过一阶段的回滚日志进行反向补偿。 大致上的逻辑就是上面这样，我们通过一个具体的案例来看看 AT 模式是如何工作的： 假设有一个业务表 product，如下： 现在我们想做如下一个更新操作： 1update product set name = 'GTS' where name = 'TXC'; 步骤如下： 一阶段： 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = ‘TXC’）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据（查找到更新之前的数据）。 执行上面的更新 SQL。 查询后镜像：根据前镜像的结果，通过主键定位数据。 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 全局锁。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 二阶段： 二阶段分两种情况，提交或者回滚。 先来看回滚步骤： 首先收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录（这条记录中保存了数据修改前后对应的镜像）。 数据校验：拿 UNDO LOG 中的后镜像与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句：update product set name = 'TXC' where id = 1; 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 再来看提交步骤： 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 大致上就是这样一个步骤，思路还是比较清晰的，就是当你要更新一条记录的时候，系统将这条记录更新之前和更新之后的内容生成一段 JSON 并存入 undo log 表中，将来要回滚的话，就根据 undo log 中的记录去更新数据（反向补偿），将来要是不回滚的话，就删除 undo log 中的记录。 在整个过程中，开发者只需要额外创建一张 undo log 表就行了，然后给需要处理全局事务的地方加上 @GlobalTransactional 注解就行了。 其他的提交呀回滚呀都是全自动的，比较省事。所以如果你项目中选择了用 seata 来处理分布式事务，那么用 AT 模式的概率还是相当高的。 5. TCC 模式TCC（Try-Confirm-Cancel） 模式就带一点手动的感觉了，它也是两阶段，但是和 AT 又不太一样，我们来看下流程。 官网上有一张 TCC 的流程图，我们来看下： 可以看到，TCC 也是分为两阶段： 第一阶段是 prepare，在这个阶段主要是做资源的检测和预留工作，例如银行转账，这个阶段就先去检查下用户的钱够不够，不够就直接抛异常，够就先给冻结上。 第二阶段是 commit 或 rollback，这个主要是等各个分支事务的一阶段都执行完毕，都执行完毕后各自将自己的情况报告给 TC，TC 一统计，发现各个分支事务都没有异常，那么就通知大家一起提交；如果 TC 发现有分支事务发生异常了，那么就通知大家回滚。 那么小伙伴可能也发现了，上面这个流程中，一共涉及到了三个方法，prepare、commit 以及 rollback，这三个方法都完全是用户自定义的方法，都是需要我们自己来实现的，所以我一开始就说 TCC 是一种手动的模式。 和 AT 相比，大家发现 TCC 这种模式其实是不依赖于底层数据库的事务支持的，也就是说，哪怕你底层数据库不支持事务也没关系，反正 prepare、commit 以及 rollback 三个方法都是开发者自己写的，我们自己将这三个方法对应的流程捋顺就行了。 6. XA 模式如果小伙伴们懂得 MySQL 数据库的 XA 事务，那么一下子就懂得 seata 中的 XA 模式是咋回事了。 XA 规范是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准。 XA 规范描述了全局的事务管理器与局部的资源管理器之间的接口。XA规范的目的是允许的多个资源（如数据库，应用服务器，消息队列等）在同一事务中访问，这样可以使 ACID 属性跨越应用程序而保持有效。 XA 规范使用两阶段提交来保证所有资源同时提交或回滚任何特定的事务。 XA 规范在上世纪 90 年代初就被提出。目前，几乎所有主流的数据库都对 XA 规范提供了支持。 XA 事务的基础是两阶段提交协议。需要有一个事务协调者来保证所有的事务参与者都完成了准备工作(第一阶段)。如果协调者收到所有参与者都准备好的消息，就会通知所有的事务都可以提交了（第二阶段）。MySQL 在这个 XA 事务中扮演的是参与者的角色，而不是协调者(事务管理器)。 MySQL 的 XA 事务分为内部 XA 和外部 XA。外部 XA 可以参与到外部的分布式事务中，需要应用层介入作为协调者；内部 XA 事务用于同一实例下跨多引擎事务，由 Binlog 作为协调者，比如在一个存储引擎提交时，需要将提交信息写入二进制日志，这就是一个分布式内部 XA 事务，只不过二进制日志的参与者是 MySQL 本身。MySQL 在 XA 事务中扮演的是一个参与者的角色，而不是协调者。 换言之，MySQL 天然的就可以通过 XA 规范来实现分布式事务，只不过需要借助一些外部应用的支持。我们来看下 Seata 中的 XA 模式使用流程。 先来看一张来自官方的图片： 可以看到，这也是一个两阶段提交： 一阶段：业务 SQL 操作放在 XA 分支中进行，XA 分支完成后，执行 XA prepare，由 RM 对 XA 协议的支持来保证持久化（即之后任何意外都不会造成无法回滚的情况）。 二阶段分两种情况：提交或者回滚： 分支提交：执行 XA 分支的 commit 分支回滚：执行 XA 分支的 rollback 和前面两种模式的区别在于，XA 模式中的回滚，是正儿八经的回滚，是我们传统意义上所理解的回滚，而不是一种反向补偿。 7. Saga 模式最后再来看看 saga 模式，这种模式应用很少，大家作为了解即可。 saga 模式是 seata 提供的长事务解决方案，然而长事务是我们在开发中应该避免的，因为效率低并且容易造成死锁。 这个 saga 模式就有点像流程引擎，开发者先自己画一个流程引擎，把整个事务中涉及到的方法都囊括进来，每一个方法返回什么的时候就是正常的，返回什么就是异常的，正常的就继续往下走，异常的就执行另一套流程，也就是我们需要提前准备好两套方法，第一套是各种正常情况的执行流程，第二套则是发生异常之后的执行流程，类似下面这样： 绿色的都是正常的流程，红色的则是发生异常后回滚的流程。回滚中也是一种反向补偿。","link":"/2022/06/01/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/seata%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"MySQL 之 两阶段提交","text":"1. 什么是两阶段提交1.1 binlog 与 redologbinlogbinlog 我们中文一般称作归档日志，当我们搭建 MySQL 主从的时候就离不开 binlog binlog 是 MySQL Server 层的日志，而不是存储引擎自带的日志，它记录了所有的 DDL 和 DML(不包含数据查询语句)语句，而且是以事件形式记录，还包含语句所执行的消耗的时间等，需要注意的是： binlog 是一种逻辑日志，他里边所记录的是一条 SQL 语句的原始逻辑，例如给某一个字段 +1，注意这个区别于 redo log 的物理日志（在某个数据页上做了什么修改）。 binlog 文件写满后，会自动切换到下一个日志文件继续写，而不会覆盖以前的日志，这个也区别于 redo log，redo log 是循环写入的，即后面写入的可能会覆盖前面写入的。 一般来说，我们在配置 binlog 的时候，可以指定 binlog 文件的有效期，这样在到期后，日志文件会自动删除，这样避免占用较多存储空间。 根据 MySQL 官方文档的介绍，开启 binlog 之后，大概会有 1% 的性能损耗，不过这还是可以接受的，一般来说，binlog 有两个重要的使用场景： MySQL 主从复制时：在主机上开启 binlog，主机将 binlog 同步给从机，从机通过 binlog 来同步数据，进而实现主机和从机的数据同步。 MySQL 数据恢复，通过使用 mysqlbinlog 工具再结合 binlog 文件，可以将数据恢复到过去的某一时刻。 redo log前面我们说的 binlog 是 MySQL 自己提供的，在 MySQL 的 server 层，而 redo log 则不是 MySQL 提供的，是存储引擎 InnoDB 自己提供的。所以在 MySQL 中就存在两类日志 binlog 和 redo log，存在两类日志既有历史原因（InnoDB 最早不是 MySQL 官方存储引擎）也有技术原因，这个咱们以后再细聊。 我们都知道，事务的四大特性里面有一个是持久性，即只要事务提交成功，那么对数据库做的修改就被永久保存下来了，写到磁盘中了，怎么做到的呢？其实我们很容易想到是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中，一旦写到磁盘中，就不怕数据丢失了。 但是要是每次都这么搞，数据库就不知道慢到哪里去了！因为 Innodb 是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，不仅效率低，也浪费资源。效率低是因为这些数据页在物理上并不连续，将数据页刷到磁盘会涉及到随机 IO。 有鉴于此，MySQL 设计了 redo log，在 redo log 中只记录事务对数据页做了哪些修改。那有人说，写 redo log 不就是磁盘 IO 吗？而写数据到磁盘也是磁盘 IO，既然都是磁盘 IO，那干嘛不把直接把数据写到磁盘呢？还费这事！ 此言差矣。 写 redo log 跟写数据有一个很大的差异，那就是 redo log 是顺序 IO，而写数据涉及到随机 IO，写数据需要寻址，找到对应的位置，然后更新/添加/删除，而写 redo log 则是在一个固定的位置循环写入，是顺序 IO，所以速度要高于写数据。 redo log 本身又分为： 日志缓冲（redo log buffer)，该部分日志是易失性的。 重做日志(redo log file)，这是磁盘上的日志文件，该部分日志是持久的。 MySQL 每执行一条 DML 语句，先将记录写入 redo log buffer，后续在某个时间点再一次性将多个操作记录写到 redo log file，这种先写日志再写磁盘的技术就是 MySQL 里经常说到的 WAL(Write-Ahead Logging) 技术（预写日志）。 1.2 两阶段提交在 MySQL 中，两阶段提交的主角就是 binlog 和 redolog，我们来看一个两阶段提交的流程图： 从上图中可以看出，在最后提交事务的时候，有 3 个步骤： 写入 redo log，处于 prepare 状态。 写 binlog。 修改 redo log 状态变为 commit。 由于 redo log 的提交分为 prepare 和 commit 两个阶段，所以称之为两阶段提交。 2. 为什么需要两阶段提交如果没有两阶段提交，那么 binlog 和 redolog 的提交，无非就是两种形式： 先写 binlog 再写 redolog。 先写 redolog 再写 binlog。 这两种情况我们分别来看。 假设我们要向表中插入一条记录 R，如果是先写 binlog 再写 redolog，那么假设 binlog 写完后崩溃了，此时 redolog 还没写。那么重启恢复的时候就会出问题：binlog 中已经有 R 的记录了，当从机从主机同步数据的时候或者我们使用 binlog 恢复数据的时候，就会同步到 R 这条记录；但是 redolog 中没有关于 R 的记录，所以崩溃恢复之后，插入 R 记录的这个事务是无效的，即数据库中没有该行记录，这就造成了数据不一致。 相反，假设我们要向表中插入一条记录 R，如果是先写 redolog 再写 binlog，那么假设 redolog 写完后崩溃了，此时 binlog 还没写。那么重启恢复的时候也会出问题：redolog 中已经有 R 的记录了，所以崩溃恢复之后，插入 R 记录的这个事务是有效的，通过该记录将数据恢复到数据库中；但是 binlog 中还没有关于 R 的记录，所以当从机从主机同步数据的时候或者我们使用 binlog 恢复数据的时候，就不会同步到 R 这条记录，这就造成了数据不一致。 那么按照前面说的两阶段提交就能解决问题吗？ 我们来看如下三种情况： 情况一：一阶段提交之后崩溃了，即写入 redo log，处于 prepare 状态 的时候崩溃了，此时： 由于 binlog 还没写，redo log 处于 prepare 状态还没提交，所以崩溃恢复的时候，这个事务会回滚，此时 binlog 还没写，所以也不会传到备库。 情况二：假设写完 binlog 之后崩溃了，此时： redolog 中的日志是不完整的，处于 prepare 状态，还没有提交，那么恢复的时候，首先检查 binlog 中的事务是否存在并且完整，如果存在且完整，则直接提交事务，如果不存在或者不完整，则回滚事务。 情况三：假设 redolog 处于 commit 状态的时候崩溃了，那么重启后的处理方案同情况二。 由此可见，两阶段提交能够确保数据的一致性。","link":"/2022/06/01/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"},{"title":"rocketmq","text":"一、消息队列1.1 介绍消息队列中间件是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题。实现高性能， 高可用，可伸缩和最终一致性架构。是大型分布式系统不可缺少的中间件。 目前在生产环境，使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ等。 来个小故事 12345678910111213141516171819202122有一天，产品跑来说：“我们要做一个用户注册功能，需要在用户注册成功后给用户发一封成功邮件。”小明（攻城狮）：“好，需求很明确了。” 不就提供一个注册接口，保存用户信息，同时发起邮件调用，待邮件发送成功后，返回用户操作成功。没一会功夫，代码就写完了。验证功能没问题后，就发布上线了。线上正常运行了一段时间，产品匆匆地跑来说：“你做的功能不行啊，运营反馈注册操作响应太慢，已经有好多用户流失了。”小明听得一身冷汗，赶紧回去改。他发现，原先的以单线程同步阻塞的方式进行邮件发送，确实存在问题。这次，他利用了 JAVA 多线程的特性，另起线程进行邮件发送，主线程直接返回保存结果。测试通过后，赶紧发布上线。小明心想，这下总没问题了吧。没过多久，产品又跑来了，他说：“现在，注册操作响应是快多了。但是又有新的问题了，有用户反应，邮件收不到。能否在发送邮件时，保存一下发送的结果，对于发送失败的，进行补发。”小明一听，哎，又得熬夜加班了。产品看他一脸苦逼的样子，忙说：“邮件服务这块，别的团队都已经做好了，你不用再自己搞了，直接用他们的服务。”小明赶紧去和邮件团队沟通，谁知他们的服务根本就不对外开放。这下小明可开始犯愁了，明知道有这么一个服务，可是偏偏又调用不了。邮件团队的人说，“看你愁的，我给你提供了一个类似邮局信箱的东西，你往这信箱里写上你要发送的消息，以及我们约定的地址。之后你就不用再操心了，我们自然能从约定的地址中取得消息，进行邮件的相应操作。”后来，小明才知道，这就是外界广为流传的消息队列。你不用知道具体的服务在哪，如何调用。你要做的只是将该发送的消息，向你们约定好的地址进行发送，你的任务就完成了。对应的服务自然能监听到你发送的消息，进行后续的操作。这就是消息队列最大的特点，将同步操作转为异步处理，将多服务共同操作转为职责单一的单服务操作，做到了服务间的解耦。哈哈，这下能高枕无忧了。太年轻，哪有万无一失的技术啊~不久的一天，你会发现所有业务都替换了邮件发送的方式，统一使用了消息队列来进行发送。这下仅仅一个邮件服务模块，难以承受业务方源源不断的消息，大量的消息堆积在了队列中。这就需要更多的消费者（邮件服务）来共同处理队列中的消息，即所谓的分布式消息处理。 1.2 应用场景a. 电商系统 （1）应用将主干逻辑处理完成后，写入消息队列。消息发送是否成功可以开启消息的确认模式。（消息队列返回消息接收成功状态后，应用再返回，这样保障消息的完整性） （2）扩展流程（发短信，配送处理）订阅队列消息。采用推或拉的方式获取消息并处理。 （3）消息将应用解耦的同时，带来了数据一致性问题，可以采用最终一致性方式解决。比如主数据写入数据库，扩展应用根据消息队列，并结合数据库方式实现基于消息队列的后续处理。 b. 日志收集系统 (消息将应用解耦的同时，带来了数据一致性问题，可以采用最终一致性方式解决。比如主数据写入数据库，扩展应用根据消息队列，并结合数据库方式实现基于消息队列的后续处理。) 1.Zookeeper注册中心，提出负载均衡和地址查找服务； 2.日志收集客户端，用于采集应用系统的日志，并将数据推送到kafka队列； 3.Kafka集群：接收，路由，存储，转发等消息处理； 1.3 rocketmq NameServer ： 负责 Topic 和 路由信息的管理，类型 Dubblo 的 Zookeeper 无状态节点，可集群部署，节点之间无信息同步 Producer ：消息生产者，一般有业务系统负责产生消息 Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Broker ：消息中专角色，负责存储消息，转发消息 Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。 Consumer : 消息消费者，负责消费消息。 Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 1.4 rocketmq 逻辑部署图 Producer Group 用来表示一个发送消息应用，一个 Producer Group 下包含多个 Producer 实例，可以是多台机器，也可以 是一台机器的多个进程，或者一个进程的多个 Producer 对象。一个 Producer Group 可以发送多个 Topic 消息，Producer Group 作用如下： 标识一类 Producer 可以通过运维工具查询这个发送消息应用下有多个 Producer 实例 发送分布式事务消息时，如果 Producer 中途意外宕机，Broker 会主动回调 Producer Group 内的任意 一台机器来确认事务状态。 Consumer Group 用来表示一个消费消息应用，一个 Consumer Group 下包含多个 Consumer 实例，可以是多台机器，也可 以是多个进程，或者是一个进程的多个 Consumer 对象。一个 Consumer Group 下的多个 Consumer 以均摊 方式消费消息，如果设置为广播方式，那么这个 Consumer Group 下的每个实例都消费全量数据。 1.5 NameServer 路由注册，删除机制 Broker每30秒向NameServer发送心跳包，心跳包中包含topic的路由信息 NarneServer 收到 Broker 心跳包后 更新 brokerLiveTable 中的信息， 特别记录心跳时间 lastUpdateTime NarneServer 每隔 10s 扫描 brokerLiveTable， 检 测表中上次收到心跳包的时间，比较当前时间 与上一次时间，如果超过120s，则认为 broker 不可用，移除路由表中与该 broker相关的所有 信息 消息生产者拉取主题的路由信息，即消息生产者并不会立即感知 Broker 服务器的新增与删除。 1.6 消息领域模型图 Topic Topic表示消息的第一级类型，比如一个电商系统的消息可以分为：交易消息、物流消息等。一条消息必须有一个Topic。 最细粒度的订阅单位，一个Group可以订阅多个Topic的消息。 Tag Tag表示消息的第二级类型，比如交易消息又可以分为：交易创建消息，交易完成消息等。RocketMQ提供2级消息分类，方便灵活控制。 Group 组，一个组可以订阅多个Topic。 Message Queue 消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。 在 RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用 Offset 来访问，offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。 也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。 二、简单使用springboot 集成 rocketmq 2.1 添加依赖123456789101112&lt;!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-spring-boot-starter --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- lombok 插件--&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 注册中心地址 1234567server: port: 8888rocketmq: name-server: 127.0.0.1:9876 producer: group: ${spring.application.name} sendMessageTimeout: 30000 2.2 生产者定义 demo 实体类 12345678import lombok.Data;@Datapublic class Demo { private int id; private String name;} 具体 生产者 service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import com.example.rocketmq_test.entity.Demo;import lombok.extern.slf4j.Slf4j;import org.apache.rocketmq.client.producer.SendCallback;import org.apache.rocketmq.client.producer.SendResult;import org.apache.rocketmq.spring.core.RocketMQTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.messaging.support.MessageBuilder;import org.springframework.stereotype.Component;/** * rocketmq-spring-boot-starter 中 * 1. Tag 的设置方式： 在 topic后面加上 “:tagName” * 2. key 设置，标识消息唯一性 * MessageBuilder.withPayload(msgBody).setHeader(RocketMQHeaders.KEYS, &quot;key1&quot;) */@Slf4j@Componentpublic class MQProducerService { private static final String topic = &quot;MESSAGE_TOPIC&quot;; @Autowired private RocketMQTemplate rocketMQTemplate; /** * 普通发送对象 * @param demo */ public void send(Demo demo){ rocketMQTemplate.convertAndSend(topic,demo); } /** * 同步发送消息 * @param message * @return */ public SendResult syncSend(String message){ return rocketMQTemplate.syncSend(topic, MessageBuilder.withPayload(message).build()); } /** * 异步发送 带回调函数 * @param message */ public void asyncSend(String message){ rocketMQTemplate.asyncSend(topic, MessageBuilder.withPayload(message).build(), new SendCallback() { @Override public void onSuccess(SendResult sendResult) { log.info(&quot;success asyncSend:{}&quot;,sendResult); } @Override public void onException(Throwable throwable) { log.info(&quot;fail asyncSend:{}&quot;,throwable); } }); } /** * * @param message * @param delay 延时消息一共分为18个等级分别为：1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h */ public void delaySend(String message,int delay){ rocketMQTemplate.syncSend(topic, MessageBuilder.withPayload(message).build(),delay); } /** * * @param message * @param tag 发送带tag的消息 */ public SendResult syncSendWithTag(String message,String tag){ return rocketMQTemplate.syncSend(topic+&quot;:&quot;+tag,MessageBuilder.withPayload(message) .setHeader(RocketMQHeaders.KEYS, UUID.randomUUID().toString()) .build()); }} 2.3 消费者12345678910111213141516171819202122232425262728293031323334353637383940@Slf4j@Componentpublic class MQConsumerService { // topic需要和生产者的topic一致，consumerGroup属性是必须指定的，内容可以随意 // selectorExpression的意思指的就是tag，默认为“*”，不设置的话会监听所有消息 @Service @RocketMQMessageListener(topic = &quot;MESSAGE_TOPIC&quot;, selectorExpression = &quot;tag1&quot;, consumerGroup = &quot;Con_Group_One&quot;) public class consumerSend implements RocketMQListener&lt;Demo&gt;{ @Override public void onMessage(Demo demo) { log.info(&quot;consumerSend receive demo ={}&quot;, JSON.toJSONString(demo)); } }// @Service// @RocketMQMessageListener(topic = &quot;MESSAGE_TOPIC&quot;, consumerGroup = &quot;Con_Group_Two&quot;)// public class consumerSend1 implements RocketMQListener&lt;String&gt;{//// @Override// public void onMessage(String demo) { log.info(&quot;consumerSend1 receive demo ={}&quot;, demo); } } // MessageExt：是一个消息接收通配符，不管发送的是String还是对象，都可接收，当然也可以像上面明确指定类型（我建议还是指定类型较方便） @Service @RocketMQMessageListener(topic = &quot;MESSAGE_TOPIC&quot;, selectorExpression = &quot;tag2&quot;, consumerGroup = &quot;Con_Group_Three&quot;) public class Consumer implements RocketMQListener&lt;MessageExt&gt; { @Override public void onMessage(MessageExt messageExt) { byte[] body = messageExt.getBody(); String msg = new String(body); log.info(&quot;Consumer 监听到消息：msg={}&quot;, msg); } }} 以下是每个状态的说明列表： FLUSH_DISK_TIMEOUT 如果Broker设置MessageStoreConfig的FlushDiskType = SYNC_FLUSH（默认为ASYNC_FLUSH），并且Broker没有在MessageStoreConfig的syncFlushTimeout（默认为5秒）内完成刷新磁盘，您将获得此状态。 FLUSH_SLAVE_TIMEOUT 如果Broker的角色是SYNC_MASTER（默认为ASYNC_MASTER），并且从属Broker未在MessageStoreConfig的syncFlushTimeout（默认为5秒）内完成与主服务器的同步，则您将获得此状态。 SLAVE_NOT_AVAILABLE 如果Broker的角色是SYNC_MASTER（默认为ASYNC_MASTER），但没有配置slave Broker，您将获得此状态。 SEND_OK SEND_OK并不意味着它是可靠的。要确保不会丢失任何消息，还应启用SYNC_MASTER或SYNC_FLUSH。 sendStatus : SEND_OK 消费成功 12345678910111213141516171819{ &quot;code&quot;: 200, &quot;time&quot;: 1651720616801, &quot;msg&quot;: &quot;操作成功&quot;, &quot;body&quot;: { &quot;sendStatus&quot;: &quot;SEND_OK&quot;, &quot;msgId&quot;: &quot;7F000001590018B4AAC21705329D000A&quot;, &quot;messageQueue&quot;: { &quot;topic&quot;: &quot;MESSAGE_TOPIC&quot;, &quot;brokerName&quot;: &quot;localhost.localdomain&quot;, &quot;queueId&quot;: 1 }, &quot;queueOffset&quot;: 4, &quot;transactionId&quot;: null, &quot;offsetMsgId&quot;: &quot;C0A8508300002A9F0000000000027DC7&quot;, &quot;regionId&quot;: &quot;DefaultRegion&quot;, &quot;traceOn&quot;: true }} msgId：Product 生成唯一值offsetMsgId:Broker 生成唯一值 三、RocketMQ 消息重复消费场景以及解决办法消息重复消费是各个MQ都会发生的常见问题之一，在一些比较敏感的场景下，重复消费会造成比较严重的后果，比如重复扣款等。 那么在什么情况下会发生RocketMQ的消息重复消费呢？ 当系统的调用链路比较长的时候，比如系统A调用系统B，系统B再把消息发送到RocketMQ中，在系统A调用系统B的时候，如果系统B处理成功，但是迟迟没有将调用成功的结果返回给系统A的时候，系统A就会尝试重新发起请求给系统B，造成系统B重复处理，发起多条消息给RocketMQ造成重复消费 在系统B发送消息给RocketMQ的时候，也有可能会发生和上面一样的问题，消息发送超时，结果系统B重试，导致RocketMQ接收到了重复的消息 当RocketMQ成功接收到消息，并将消息交给消费者处理，如果消费者消费完成后还没来得及提交offset给RocketMQ，自己宕机或者重启了，那么RocketMQ没有接收到offset，就会认为消费失败了，会重发消息给消费者再次消费 消息重复消费的场景大概可以分为生产者端重复消费和消费者端重复消费，那么如何来解决消息的重复消费呢？ 答案是通过幂等性来保证，只要保证重复消费的消息不对结果产生影响，就完美地解决这个问题 在生产者端要保证幂等性的话，大概可以使用以下两种方式：① RocketMQ支持消息查询的功能，只要去RocketMQ查询一下是否已经发送过该条消息就可以了，不存在则发送，存在则不发送② 引入redis，在发送消息到RocketMQ成功之后，向redis中插入一条数据，如果发生重试，则先去redis中查询一下该条消息是否已经发送过了，存在的话就不重复发送消息了 生产者的这两种幂等性方案都可以实现，但是都存在一定的缺陷方案①，RocketMQ消息查询的性能不是特别好，如果是在高并发的场景下，每条消息在发送到RocketMQ时都去查询一下，可能会影响接口的性能方案②，在一些极端的场景下，redis也无法保证消息发送成功之后，就一定能写入redis成功，比如写入消息成功而redis此时宕机，那么再次查询redis判断消息是否已经发送过，是无法得到正确结果的 既然在生产者做幂等性的方案都不是特别靠谱，那就再在消费者端来做吧 消息的消费，最后都对应的是数据库的操作，只要在消息消费的时候，判断一下数据库中是否已经消费过了这条消息，就可以保证幂等性了，例如使用唯一索引，保证一条消息只被消费一次。 消息重复消费是一个非常常见的问题，在很多系统调用频繁的场景下，都可能会出现超时重试的情况，还有就是系统频繁迭代，经常重启系统更新的场景，也会出现消息重复消费 生产者端发送重复的消息到RocketMQ中其实问题不大，消息只是在RocketMQ中重复了，并没有影响到系统的数据，我们只需要在最后修改数据库的时候，保证好幂等性即可","link":"/2022/05/13/%E4%B8%AD%E9%97%B4%E4%BB%B6/rocketmq/rocketmq%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/"},{"title":"线程","text":"一、线程1.1 什么是线程线程是进程中得一个实体，线程本身是不会独立存在的。进程是代码在数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，线程则是进程的一个执行路径，一个进程中至少有一线程，进程中的多个线程共享进程的资源。 1.1 为什么需要线程池线程池是一种基于池化思想管理线程的工具，类似于数据库连接池。 基于池化的思想是提前创建一定数量的 Connection 对象，然后将其存放起来，这样每次需要的时候直接从池中获取 Connection 对象即可，避免了频繁的创建和销毁的操作。线程池则是将线程 Thread 对象提前创建缓存起来，这样当提交任务的时候直接将任务提交给池中的线程即可，而不需要创建新的 Thread。总结线程池的优势： 传统创建线程的方式对资源无限申请缺少抑制手段，容易引发资源耗尽的风险 通过复用线程池中的线程可以避免频繁创建线程，省去创建线程的时间，提高响应速度 线程池可以指定最大线程数量，超出的任务会在等待队列等待，不会出现大量线程耗尽服务器资源 提升系统响应速度,假如创建线程用的时间为T1，执行任务用的时间为T2,销毁线程用的时间为T3，那么使用线程池就免去了T1和T3的时间 二、线程池介绍2.1 ThreadPoolExecutor类继承结构如下 顶层接口 Executor 提供了一种思想：将任务提交和任务执行进行解耦。 只需要把要做的事情交给线程池就行了。 2.1.1 ThreadPoolExecutor 参数介绍1234567891011public ThreadPoolExecutor( int corePoolSize, //核心线程数量 int maximumPoolSize,// 最大线程数 long keepAliveTime, // 最大空闲时间 TimeUnit unit, // 时间单位 BlockingQueue&lt;Runnable&gt; workQueue, // 任务队列 ThreadFactory threadFactory, // 线程工厂 RejectedExecutionHandler handler // 饱和处理机制,拒绝策略 ) { ... } 2.1.2 场景代入理解 a客户(任务)去银行(线程池)办理业务,但银行刚开始营业,窗口服务员还未就位(相当于线程池中初始线程数量为0),于是经理(线程池管理者)就安排1号工作人员(创建1号线程执行任务)接待a客户(创建线程); 在a客户业务还没办完时,b客户(任务)又来了,于是经理(线程池管理者)就安排2号工作人员(创建2号线程执行任务)接待b客户(又创建了一个新的线程);假设该银行总共就2个窗口(核心线程数量是2); 紧接着在a,b客户都没有结束的情况下c客户来了,于是经理(线程池管理者)就安排c客户先坐到银行大厅的座位上(空位相当于是任务队列)等候,并告知他: 如果1、2号工作人员空出,c就可以前去办理业务; 此时d客户又到了银行,(工作人员都在忙,大厅座位也满了)于是经理赶紧安排临时工(新创建的线程)在大堂站着,手持pad设备给d客户办理业务; 假如前面的业务都没有结束的时候e客户又来了,此时正式工作人员都上了,临时工也上了,座位也满了(临时工加正式员工的总数量就是最大线程数),于是经理只能按《超出银行最大接待能力处理办法》(饱和处理机制)拒接接待e客户; 最后,进来办业务的人少了,大厅的临时工空闲时间也超过了1个小时(最大空闲时间),经理就会让这部分空闲的员工人下班.(销毁线程)但是为了保证银行银行正常工作(有一个allowCoreThreadTimeout变量控制是否允许销毁核心线程,默认false),即使正式工闲着,也不得提前下班,所以1、2号工作人员继续待着(池内保持核心线程数量); 2.1.3 线程池的工作流程 1.核心线程数(corePoolSize) 线程池中正在运行的线程数量小于 corePoolSize 时，新任务来了会创建新线程执行任务 核心线程数的设计需要依据任务的处理时间和每秒产生的任务数量来确定,例如:执行一个任务需要0.1秒,系统百分之80的时间每秒都会产生100个任务,那么要想在1秒内处理完这100个任务,就需要10个线程,此时我们就可以设计核心线程数为10; 当然实际情况不可能这么平均,所以我们一般按照80/20原则设计即可,既按照百分之80的情况设计核心线程数,剩下的百分之20可以利用最大线程数处理; 2.任务队列长度(workQueue) 用于保存等执行的任务的阻塞队列，比如基于数组的有界队列ArrayBlockingQueue、基于链表的无界LinkedBlockingQueue、和最多只有一个元素的同步队列SynchronousQueue等 任务队列长度一般设计为:核心线程数/单个任务执行时间*2即可;例如上面的场景中,核心线程数设计为10,单个任务执行时间为0.1秒,则队列长度可以设计为200; 3.最大线程数(maximumPoolSize) 线程池中允许的最大线程数。如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于maximumPoolSize； 最大线程数的设计除了需要参照核心线程数的条件外,还需要参照系统每秒产生的最大任务数决定:例如:上述环境中,如果系统每秒最大产生的任务是1000个,那么,最大线程数=(最大任务数-任务队列长度)单个任务执行时间;既: 最大线程数=(1000-200)0.1=80个; 4.最大空闲时间(keepAliveTime) 线程空闲时的存活时间，即当线程没有任务执行时，继续存活的时间；默认情况下，该参数只在线程数大于corePoolSize时才有用； 这个参数的设计完全参考系统运行环境和硬件压力设定,没有固定的参考值,用户可以根据经验和系统产生任务的时间间隔合理设置一个值即可; 5. 拒绝策略(RejectedExecutionHandler) 当线程池中的工作线程数已经达到了 maximumPoolSize，此时再提交任务就会被拒绝，JDK 提供了四种拒绝策略 AbortPolicy 丢弃任务并抛出RejectedExecutionException 异常。这是线程池默认策略，如果是比较关键的业务推荐使用此策略，这样在子系统不能承载更大并发的时候，能够及时通过异常发现 DiscardPolicy 丢弃任务但是不抛出异常。使用此策略可能会使我们无法发现系统的异常，一般无关紧要的业务使用此策略 DiscardOldestPolicy 丢弃队列最前面的任务， 也就是最早的任务，然后重新提交被拒绝的任务 CallerRunsPolicy 由调用线程（提交任务的线程）处理该任务。相当于是把任务交给主线程去执行，这样由于主线程在执行任务就会被阻塞，就不会继续给线程池提交任务，在某些情况下来说这也是个不错的拒绝策略。 当然我们也可以实现自己的拒绝策略，只需要实现 RejectedExecutionHandler 重写拒绝逻辑即可。 第三方实现的拒绝策略：Dubbo、Netty、ActiveMQ、PinPoint等均有各自实现的拒绝策略 2.1.4 线程池的状态 RUNNING ：运行状态。能接受新提交的任务，并且也能处理阻塞队列中的任务 SHUTDOWN ： 关闭状态。不再接受新提交的任务，能处理阻塞队列中的已经保存的任务 STOP ： 停止状态。不能接受新提交的任务，也不能处理阻塞队列中的任务，会中断正在处理任务的线程 TIDYING ： 所有任务都终止了，workerCount （工作线程数）为 0 TERMINATED ： 终止状态。terminated() 方法执行完进入该状态。 拒绝策略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package cn.com.winning.bg.test.thread;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;public class ThreadRejectedExecutionHandler implements RejectedExecutionHandler{ @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {} /** * 饱和策略一：调用者线程执行策略 * 在该策略下，在调用者中执行被拒绝任务的run方法。除非线程池showdown，否则直接丢弃线程 */ public static class CallerRunsPolicy extends ThreadRejectedExecutionHandler { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { //判断线程池是否在正常运行，如果线程池在正常运行则由调用者线程执行被拒绝的任务。如果线程池停止运行，则直接丢弃该任务 if (!executor.isShutdown()){ r.run(); } } } /** * 饱和策略二：终止策略 * 在该策略下，丢弃被拒绝的任务，并抛出拒绝执行异常 */ public static class AbortPolicy extends ThreadRejectedExecutionHandler { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { throw new RejectedExecutionException(&quot;请求任务：&quot; + r.toString() + &quot;，线程池负载过高执行饱和终止策略！&quot;); } } /** * 饱和策略三：丢弃策略 * 在该策略下，什么都不做直接丢弃被拒绝的任务 */ public static class DiscardPolicy extends ThreadRejectedExecutionHandler { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {} } /** * 饱和策略四：弃老策略 * 在该策略下，丢弃最早放入阻塞队列中的线程，并尝试将拒绝任务加入阻塞队列 */ public static class DiscardOldestPolicy extends ThreadRejectedExecutionHandler { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { //判断线程池是否正常运行，如果线程池正常运行则弹出（或丢弃）最早放入阻塞队列中的任务，并尝试将拒绝任务加入阻塞队列。如果线程池停止运行，则直接丢弃该任务 if (!executor.isShutdown()){ executor.getQueue().poll(); executor.execute(r); } } }} 线程创建工厂类 1234567891011121314package cn.com.winning.bg.test.thread;import java.util.concurrent.ThreadFactory;public class MyThreadFactory implements ThreadFactory{ @Override public Thread newThread(Runnable r) { Thread newThread = new Thread(r); return newThread; }} 线程池工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package cn.com.winning.bg.test.thread;import java.util.List;import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * 线程池工具类 * ClassName: ThreadPoolUtils * @Description: TODO * @date 2022年4月19日 */public class ThreadPoolUtils { /** * 系统可用计算资源 */ private static final int CPU_COUNT = Runtime.getRuntime().availableProcessors(); /** * 核心线程数 */ private static final int CORE_POOL_SIZE = Math.max(2, Math.min(CPU_COUNT - 1, 4)); /** * 最大线程数 */ private static final int MAXIMUM_POOL_SIZE = CPU_COUNT * 2 + 1; /** * 空闲线程存活时间 */ private static final int KEEP_ALIVE_SECONDS = 30; /** * 工作队列 */ private static final BlockingQueue&lt;Runnable&gt; POOL_WORK_QUEUE = new LinkedBlockingQueue&lt;&gt;(128); /** * 工厂模式 */ private static final MyThreadFactory MY_THREAD_FACTORY = new MyThreadFactory(); /** * 饱和策略 */ private static final ThreadRejectedExecutionHandler THREAD_REJECTED_EXECUTION_HANDLER = new ThreadRejectedExecutionHandler.CallerRunsPolicy(); /** * 线程池对象 */ private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR; /** * 声明式定义线程池工具类对象静态变量，在所有线程中同步 */ private static volatile ThreadPoolUtils threadPoolUtils = null; /** * 初始化线程池静态代码块 */ static { THREAD_POOL_EXECUTOR = new ThreadPoolExecutor( //核心线程数 CORE_POOL_SIZE, //最大线程数 MAXIMUM_POOL_SIZE, //空闲线程执行时间 KEEP_ALIVE_SECONDS, //空闲线程执行时间单位 TimeUnit.SECONDS, //工作队列（或阻塞队列） POOL_WORK_QUEUE, //工厂模式 MY_THREAD_FACTORY, //饱和策略 THREAD_REJECTED_EXECUTION_HANDLER ); } /** * 线程池工具类空参构造方法 */ private ThreadPoolUtils() {} /** * 获取线程池工具类实例 * @return */ public static ThreadPoolUtils getNewInstance(){ if (threadPoolUtils == null) { synchronized (ThreadPoolUtils.class) { if (threadPoolUtils == null) { threadPoolUtils = new ThreadPoolUtils(); } } } return threadPoolUtils; } /** * 执行线程任务 * @param runnable 任务线程 */ public void executor(Runnable runnable) { THREAD_POOL_EXECUTOR.execute(runnable); } /** * 获取线程池状态 * @return 返回线程池状态 */ public boolean isShutDown(){ return THREAD_POOL_EXECUTOR.isShutdown(); } /** * 停止正在执行的线程任务 * @return 返回等待执行的任务列表 */ public List&lt;Runnable&gt; shutDownNow(){ return THREAD_POOL_EXECUTOR.shutdownNow(); } /** * 关闭线程池 */ public void showDown(){ THREAD_POOL_EXECUTOR.shutdown(); } /** * 关闭线程池后判断所有任务是否都已完成 * @return */ public boolean isTerminated(){ return THREAD_POOL_EXECUTOR.isTerminated(); }} 2.2 ForkJoinPool类继承结构 2.2.1 ForkJoinPool 是什么ForkJoinPool是自java7开始，jvm提供的一个用于并行执行的任务框架。其主旨是将大任务分成若干小任务，之后再并行对这些小任务进行计算，最终汇总这些任务的结果。得到最终的结果。其广泛用在java8的stream中。 2.2.2 分治法 分治法的基本思想是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题的相互独立且与原问题的性质相同，求出子问题的解之后，将这些解合并，就可以得到原有问题的解。是一种分目标完成的程序算法。简单的问题，可以用二分法来完成。 2.2.3 工作窃取 工作窃取是指当某个线程的任务队列中没有可执行任务的时候，从其他线程的任务队列中窃取任务来执行，以充分利用工作线程的计算能力，减少线程由于获取不到任务而造成的空闲浪费。在ForkJoinpool中，工作任务的队列都采用双端队列Deque容器。我们知道，在通常使用队列的过程中，我们都在队尾插入，而在队头消费以实现FIFO。而为了实现工作窃取。一般我们会改成工作线程在工作队列上LIFO,而窃取其他线程的任务的时候，从队列头部取获取 2.2.4 简单使用加法计算 主要逻辑 123456789101112131415161718192021222324252627282930313233import java.util.concurrent.RecursiveTask;public class CountTask extends RecursiveTask&lt;Integer&gt;{ private static final long serialVersionUID = 1L; private static final int THRESHOLD = 49; private int start; private int end; public CountTask(int start, int end) { this.start = start; this.end = end; } @Override protected Integer compute() { if (end - start &lt;= THRESHOLD) { int result = 0; for (int i = start; i &lt;= end; i++) { result += i; } return result; } else { int middle = (start + end) / 2; CountTask firstTask = new CountTask(start, middle); CountTask secondTask = new CountTask(middle + 1, end); invokeAll(firstTask,secondTask); return firstTask.join() + secondTask.join(); } }} 使用调用 123456789101112131415161718192021import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.TimeUnit;public class ForkJoinPoolTest { public static void main(String[] args) throws InterruptedException, ExecutionException { int result1 = 0; for (int i = 1; i &lt;= 1000000; i++) { result1 += i; } System.out.println(&quot;循环计算 1-1000000 累加值：&quot; + result1); ForkJoinPool pool = new ForkJoinPool(); ForkJoinTask&lt;Integer&gt; task = pool.submit(new CountTask(1, 1000000)); int result2 = task.get(); System.out.println(&quot;并行计算 1-1000000 累加值：&quot; + result2); pool.awaitTermination(2, TimeUnit.SECONDS); pool.shutdown(); }}","link":"/2022/04/27/%E5%85%B6%E4%BB%96/%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%BB%8B%E7%BB%8D/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"springcloud","slug":"springcloud","link":"/tags/springcloud/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"工具类","slug":"工具类","link":"/tags/%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"name":"限流器","slug":"限流器","link":"/tags/%E9%99%90%E6%B5%81%E5%99%A8/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"中间件","slug":"中间件","link":"/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"kaptcha","slug":"kaptcha","link":"/tags/kaptcha/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"线程","slug":"线程","link":"/tags/%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"springcloud","slug":"springcloud","link":"/categories/springcloud/"},{"name":"springboot","slug":"springboot","link":"/categories/springboot/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"工具类","slug":"工具类","link":"/categories/%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"name":"需求案例","slug":"需求案例","link":"/categories/%E9%9C%80%E6%B1%82%E6%A1%88%E4%BE%8B/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ORM","slug":"ORM","link":"/categories/ORM/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"开源框架","slug":"开源框架","link":"/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MyBatis","slug":"ORM/MyBatis","link":"/categories/ORM/MyBatis/"},{"name":"nginx","slug":"中间件/nginx","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/nginx/"},{"name":"rocketmq","slug":"中间件/rocketmq","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/rocketmq/"},{"name":"Load Balance","slug":"中间件/Load-Balance","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Load-Balance/"},{"name":"kaptcha","slug":"开源框架/kaptcha","link":"/categories/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/kaptcha/"},{"name":"MongoDB","slug":"数据库/MongoDB","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"},{"name":"MySQL","slug":"数据库/MySQL","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}]}